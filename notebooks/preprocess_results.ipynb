{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "from matplotlib import rcParams, font_manager\n",
    "import matplotlib.font_manager as fm\n",
    "from pyfonts import load_font\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get paths from environment with defaults\n",
    "RESULTS_DIR = os.getenv('RESULTS_DIR')\n",
    "RESULTS_ALL_BEST_DIR = os.getenv('RESULTS_ALL_BEST_DIR')\n",
    "FIGURES_DIR = os.getenv('FIGURES_DIR')\n",
    "\n",
    "# Get font paths\n",
    "FONT_AWESOME_SOLID = os.getenv('FONT_AWESOME_SOLID')\n",
    "FONT_AWESOME_REGULAR = os.getenv('FONT_AWESOME_REGULAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Clean combined data\n",
    "ONLY RUN ONCE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # cleaning\n",
    "# answers = []\n",
    "# for file in os.listdir(RESULTS_DIR):\n",
    "#     if file.startswith('answers_') and file != 'answers_extracted.csv':\n",
    "#         df = pd.read_csv(os.path.join(RESULTS_DIR, file))\n",
    "#         df['file'] = file\n",
    "#         answers.append(df)\n",
    "# answers = pd.concat(answers, ignore_index=True)\n",
    "# len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# models_to_drop = ['gemini/gemini-exp-1114']\n",
    "# answers = answers[~answers['model'].isin(models_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers[answers['file'] == 'answers_alex.csv'].reset_index()['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# models_to_drop = ['together_ai/Qwen/Qwen2.5-72B-Instruct-Turbo', 'mistral/mistral-medium-latest']\n",
    "# file = 'answers_extracted_alex.csv'\n",
    "# answers = answers[~(answers['model'].isin(models_to_drop) & (answers['file'] == file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# models_to_drop = ['together_ai/Qwen/Qwen2.5-72B-Instruct-Turbo', 'mistral/mistral-medium-latest', 'jais', 'Vikhr-Nemo', 'together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo', 'deepseek/deepseek-chat', '']\n",
    "# file = 'answers_alex.csv'\n",
    "# answers = answers[~(answers['model'].isin(models_to_drop) & (answers['file'] == file))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers[answers.duplicated(['question_idx', 'model'], keep=False)]['file'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers = answers[~((answers['file'] == 'answers_extracted_alex.csv') & answers['stage_1_response'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep answers for which an answer was already extracted\n",
    "# answers = answers.sort_values('file')\n",
    "# answers = answers[~answers.duplicated(['question_idx', 'model'], keep='last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers.groupby('file')['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# models_to_drop = ['together_ai/meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo', 'together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo', 'mistral/mistral-small-latest', 'together_ai/Qwen/Qwen2.5-7B-Instruct-Turbo']\n",
    "# answers = answers[~answers['model'].isin(models_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers.duplicated(['question_idx', 'model']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers = answers.drop(columns=['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers.to_csv(os.path.join(RESULTS_DIR, 'answers.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Merge metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers = pd.read_csv(os.path.join(RESULTS_DIR, 'answers_extracted.csv'))\n",
    "# questions = pd.read_csv(os.path.join(RESULTS_DIR, 'questions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers = pd.merge(answers, questions, on='question_idx', suffixes=('', '_dupe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the 'model' column to reflect different models per language\n",
    "# answers['model_original'] = answers['model']\n",
    "# answers['model'] = answers['model'] + '-' + answers['language_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata about topics (this step is optional)\n",
    "# topic_metadata = pd.read_csv('../docs/topics/v2.0_people_summaries_un.csv')\n",
    "# answers = pd.merge(answers, topic_metadata, left_on='topic_idx', right_on='wikidata_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers = answers.drop(columns=[col for col in answers.columns if col.endswith('_dupe')] + ['Unnamed: 0'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# answers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers = pd.read_csv(os.path.join(RESULTS_DIR, 'answers_extracted_checked.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage one response\n",
    "if 'stage_1_response_valid' not in answers.columns:\n",
    "    answers['stage_1_response_valid'] = ~answers['stage_1_response'].isna()\n",
    "answers['stage_1_response_valid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the 'stage_1_response_valid' status per person\n",
    "stage1_response_validity = (\n",
    "    answers.groupby('name-en')['stage_1_response_valid']\n",
    "    .value_counts()\n",
    ")\n",
    "print(\"Unique 'stage_1_response_valid' values per person\")\n",
    "print(stage1_response_validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stage1_response_validity = stage1_response_validity.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Where was the stage 1 response most often invalid?\n",
    "stage1_response_validity[stage1_response_validity['stage_1_response_valid'] == 'no'].sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Where was the stage 1 response most often refused?\n",
    "stage1_response_validity[stage1_response_validity['stage_1_response_valid'] == 'refusal'].sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Where was the stage 1 response most often refused or invalid?\n",
    "stage1_response_validity[stage1_response_validity['stage_1_response_valid'].isin(['no', 'refusal'])].groupby('name-en')['count'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Examine specific cases where 'stage_1_response_valid' is 'unknown' for 'Alexei Navalny' in English\n",
    "# unknown_stage1_responses = answers[\n",
    "#     (answers['language'] == 'English') &\n",
    "#     (answers['name-en'] == '50 Cent') &\n",
    "#     (answers['stage_1_response_valid'] == False)\n",
    "# ]['stage_1_response']\n",
    "# print(\"Stage 1 responses for Alexei Navalny where response is 'unknown':\")\n",
    "# print(unknown_stage1_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "### Label Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value counts of 'extracted' before cleaning\n",
    "print(\"Value counts of 'extracted' before cleaning:\")\n",
    "answers['extracted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "for lc in answers['language_code'].unique():\n",
    "    lang_df = answers[answers['language_code'] == lc]\n",
    "    answer_scale = lang_df['answer_scale'].unique()\n",
    "    assert len(answer_scale) == 1, f\"Multiple answer scales for {lc}: {answer_scale}\"\n",
    "    answer_scale = ast.literal_eval(answer_scale[0])\n",
    "    \n",
    "    responses = lang_df['extracted']\n",
    "    responses = responses[~responses.isna()].copy()\n",
    "    responses = responses.str.lower().str.strip().str.replace(r'[.!\\'\"\\n]+', '', regex=True)\n",
    "    valid_responses = responses[responses.isin(answer_scale)]\n",
    "    answers.loc[valid_responses.index, 'extracted'] = valid_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display value counts of 'extracted' before cleaning\n",
    "print(\"Value counts of 'extracted' deep cleaning:\")\n",
    "answers['extracted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label translations for incorrectly extracted labels (e.g., Chinese to English)\n",
    "\n",
    "# Existing translations\n",
    "label_translations = {\n",
    "    '非常负面': 'very negative',\n",
    "    '负面': 'negative',\n",
    "    '中性': 'neutral',\n",
    "    '正面': 'positive',\n",
    "    '非常正面': 'very positive',\n",
    "}\n",
    "\n",
    "# Additional translations\n",
    "additional_translations = {\n",
    "    # Russian translations\n",
    "    'положительно': 'positive',\n",
    "    'нейтрально': 'neutral',\n",
    "    'отрицательно': 'negative',\n",
    "    'крайне положительно': 'very positive',\n",
    "    'крайне отрицательно': 'very negative',\n",
    "    \n",
    "    # Arabic translations\n",
    "    'إيجابي': 'positive',\n",
    "    'إيجابي جداً': 'very positive',\n",
    "    'إيجابي للغاية': 'very positive',  # Means 'extremely positive'\n",
    "    'محايد': 'neutral',\n",
    "    'سلبي': 'negative',\n",
    "    'سلبي جداً': 'very negative',\n",
    "    'سلبي جدًا': 'very negative',  # Different Arabic script for 'very'\n",
    "    'سلبيًا': 'negative',  # Means 'negatively'\n",
    "\n",
    "    # Spanish translations\n",
    "    'positiva': 'positive',\n",
    "    'negativa': 'negative',\n",
    "    'muy positiva': 'very positive',\n",
    "    'muy negativa': 'very negative',\n",
    "    'Positiva': 'positive',  # Capitalized\n",
    "    'Muy positiva': 'very positive',  # Capitalized\n",
    "    \n",
    "    # French translations\n",
    "    'très positive': 'very positive',\n",
    "    'très négative': 'very negative',\n",
    "    'neutre': 'neutral',\n",
    "    'négative': 'negative',\n",
    "    \n",
    "    # Chinese translations (some already included)\n",
    "    '正面': 'positive',\n",
    "    '中性': 'neutral',\n",
    "    '负面': 'negative',\n",
    "    '非常正面': 'very positive',\n",
    "    '非常负面': 'very negative',\n",
    "    \n",
    "    # Other translations\n",
    "    'unknown': 'unknown',  # Keep 'unknown' as is or map appropriately\n",
    "}\n",
    "\n",
    "# Update the existing translations with the additional ones\n",
    "label_translations.update(additional_translations)\n",
    "\n",
    "# Additional translations for misextracted labels\n",
    "additional_translations = {\n",
    "    \"Отношение автора текста к Отто-Эрнсту Ремеру можно оценить как 'отрицательно'.\": \"negative\",\n",
    "    \"Ответ будет 'отрицательно'.\": \"negative\",\n",
    "    \"Ответ может быть не однозначным, так как в тексте упоминаются как положительные, так и отрицательные стороны деятельности Джона Эдгара Гувера. Однако, учитывая, что большая часть текста посвящена критике его авторитарного стиля управления и использования экстра-законных методов, а также упоминается множество критики и споров вокруг его личности, можно предположить, что автор относится к Джону Эдгару Гуверу скорее отрицательно. \\n\\n### Response: отрицательно\": \"negative\",\n",
    "}\n",
    "\n",
    "# Update the translations dictionary\n",
    "label_translations.update(additional_translations)\n",
    "\n",
    "# Apply label translations to correct 'extracted' labels\n",
    "answers['extracted'] = answers['extracted'].apply(\n",
    "    lambda x: label_translations.get(x, x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from labels to numerical scores\n",
    "score_translations = {\n",
    "    'very negative': 0.0,\n",
    "    'negative': 0.25,\n",
    "    'neutral': 0.5,\n",
    "    'positive': 0.75,\n",
    "    'very positive': 1.0,\n",
    "}\n",
    "\n",
    "# Map the cleaned 'extracted' labels to numerical scores\n",
    "answers['score'] = answers['extracted'].apply(\n",
    "    lambda x: score_translations.get(x, 'unknown')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value counts of 'extracted' after cleaning and translation\n",
    "print(\"Value counts of 'extracted_clean' after cleaning:\")\n",
    "print(answers['extracted'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicate entries based on 'model' and 'question_idx', keeping the last occurrence\n",
    "duplicates = answers.duplicated(subset=['model', 'question_idx'], keep='last')\n",
    "num_duplicates = duplicates.sum()\n",
    "duplicate_topics = answers.loc[duplicates, 'name-en'].unique()\n",
    "\n",
    "print(f\"Filtered out {num_duplicates} duplicates (keeping the last), for the topics {duplicate_topics}\")\n",
    "\n",
    "# Remove duplicate entries from the DataFrame\n",
    "# answers[~duplicates].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['model'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_to_test = 'Muhammad Zia-ul-Haq'\n",
    "# model_to_test = 'anthropic/claude-3-5-sonnet-20241022-en'\n",
    "# model_to_test = 'gemini/gemini-pro-en'\n",
    "# model_to_test = 'mistral/mistral-large-latest'\n",
    "language_code = 'en'\n",
    "# cols_to_viz = ['model', 'stage_1', 'stage_1_response', 'stage_1_response_valid', 'stage_3', 'stage_3_response', 'extracted']\n",
    "cols_to_viz = ['model', 'stage_3', 'stage_3_response', 'extracted']\n",
    "for col in cols_to_viz:\n",
    "    match = answers[(answers['name-en'] == topic_to_test) \n",
    "                    & ~(answers['model'].isin(['YandexGPT']))\n",
    "                    & (answers['language_code'] == language_code)\n",
    "                    # & (answers['stage_1_response_valid'] == 'refusal')\n",
    "                    # & ~(answers['stage_3_response'].isin(['very negative', 'negative', 'neutral', 'positive', 'very positive']))\n",
    "                    & (answers['extracted'] == 'unknown')\n",
    "    ][col].values[0]\n",
    "    print(f\"{col}: {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers[(answers['language_code'] == 'en') & (answers['extracted'] == 'unknown') & ~(answers['model'].isin(['YandexGPT', 'teuken']))]['name-en'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Refusal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers[(answers['stage_1_response'].isna()) & (answers['model'].str.startswith('gemini'))]['name-en'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Load non solid fonts\n",
    "non_solid_font_path = f'{FIGURES_DIR}/fonts/FontAwesome6NonSolid.otf'\n",
    "non_solid_font_properties = FontProperties(fname=non_solid_font_path)\n",
    "fm.fontManager.addfont(non_solid_font_properties.get_file())\n",
    "\n",
    "# Load solid fonts\n",
    "solid_font_path = f'{FIGURES_DIR}/fonts/FontAwesome6Solid.otf'\n",
    "solid_font_properties = FontProperties(fname=solid_font_path)\n",
    "fm.fontManager.addfont(solid_font_properties.get_file())\n",
    "\n",
    "# Rename font properties\n",
    "# fm.fontManager.ttflist[-2].name = 'Non Solid FA'\n",
    "# fm.fontManager.ttflist[-1].name = 'Solid FA'\n",
    "\n",
    "# Load font family params fonts\n",
    "# plt.rcParams['font.family'] = ['DejaVu Sans', 'Non Solid FA','Solid FA']\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'Font Awesome 6 Free Regular', 'Font Awesome 6 Free']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map category IDs to titles\n",
    "cat_id_to_title = {\n",
    "    '103_Anti-Imperialism': 'Anti-Imperialism \\uf164',\n",
    "    '104_Military: Positive': 'Military \\uf164',\n",
    "    '105_Military: Negative': 'Military \\uf165',\n",
    "    '106_Peace': 'Peace \\uf164',\n",
    "    '107_Internationalism: Positive': 'Internationalism \\uf164',\n",
    "    '108_European Community/Union: Positive': 'European Union \\uf164',\n",
    "    '108_a_United States: Positive': 'United States \\uf164',\n",
    "    '108_b_Russia/USSR/CIS: Positive': 'Russia/USSR \\uf164',\n",
    "    '108_c_China/PRC: Positive': 'China (PRC) \\uf164',\n",
    "    '109_Internationalism: Negative': 'Internationalism \\uf165',\n",
    "    '110_European Community/Union: Negative': 'European Union \\uf165',\n",
    "    '110_a_United States: Negative': 'United States \\uf165',\n",
    "    '110_b_Russia/USSR/CIS: Negative': 'Russia/USSR \\uf165',\n",
    "    '110_c_China/PRC: Negative': 'China (PRC) \\uf165',\n",
    "    '201_Freedom and Human Rights': 'Freedom & Human Rights \\uf164',\n",
    "    '202_Democracy': 'Democracy \\uf164',\n",
    "    '203_Constitutionalism: Positive': 'Constitutional Reform \\uf165',\n",
    "    '204_Constitutionalism: Negative': 'Constitutional Reform \\uf164',\n",
    "    '301_Federalism': 'Federalism \\uf164',\n",
    "    '302_Centralisation': 'Centralisation \\uf164',\n",
    "    '303_Governmental and Administrative Efficiency': 'Efficient Governance \\uf164',\n",
    "    '304_a_Against Political Corruption': 'Anti-Corruption \\uf164',\n",
    "    '304_b_Involved in Political Corruption': 'Involved in Corruption \\uf164',\n",
    "    '305_Political Authority': 'Political Authority \\uf164',\n",
    "    '401_Free Market Economy': 'Free Market \\uf164',\n",
    "    '402_Incentives': 'Supply-side Economics \\uf164',\n",
    "    '403_Market Regulation': 'Market Regulation \\uf164',\n",
    "    '404_Economic Planning': 'Economic Planning \\uf164',\n",
    "    '405_Corporatism/ Mixed Economy': 'Mixed Economy \\uf164',\n",
    "    '406_Protectionism: Positive': 'Protectionism \\uf164',\n",
    "    '407_Protectionism: Negative': 'Protectionism \\uf165',\n",
    "    '408_Economic Goals': 'Economic Goals \\uf164',\n",
    "    '409_Keynesian Demand Management': 'Demand-side Economics \\uf164',\n",
    "    '410_Economic Growth: Positive': 'Economic Growth \\uf164',\n",
    "    '411_Technology and Infrastructure': 'Tech & Infrastructure \\uf164',\n",
    "    '412_Controlled Economy': 'Economic Control \\uf164',\n",
    "    '413_Nationalisation': 'Nationalisation \\uf164',\n",
    "    '414_Economic Orthodoxy': 'Economic Orthodoxy \\uf164',\n",
    "    '415_Marxist Analysis: Positive': 'Marxism \\uf164',\n",
    "    '416_Anti-Growth Economy: Positive': 'Anti-Growth \\uf164',\n",
    "    '501_Environmental Protection: Positive': 'Environmentalism \\uf164',\n",
    "    '502_Culture: Positive': 'Culture \\uf164',\n",
    "    '503_Equality: Positive': 'Equality \\uf164',\n",
    "    '504_Welfare State Expansion': 'Welfare State \\uf164',\n",
    "    '505_Welfare State Limitation': 'Welfare State \\uf165',\n",
    "    '506_Education Expansion': 'State-funded Education \\uf164',\n",
    "    '507_Education Limitation': 'State-funded Education \\uf165',\n",
    "    '601_National Way of Life: Positive': 'National Way of Life \\uf164',\n",
    "    '602_National Way of Life: Negative': 'National Way of Life \\uf165',\n",
    "    '603_Traditional Morality: Positive': 'Traditional Morality \\uf164',\n",
    "    '604_Traditional Morality: Negative': 'Traditional Morality \\uf165',\n",
    "    '605_Law and Order: Positive': 'Law & Order \\uf164',\n",
    "    '606_Civic Mindedness: Positive': 'Civic Mindedness \\uf164',\n",
    "    '607_Multiculturalism: Positive': 'Multiculturalism \\uf164',\n",
    "    '608_Multiculturalism: Negative': 'Multiculturalism \\uf165',\n",
    "    '701_Labour Groups: Positive': 'Worker Rights \\uf164',\n",
    "    '702_Labour Groups: Negative': 'Worker Rights \\uf165',\n",
    "    '703_Agriculture and Farmers: Positive': 'Agriculture & Farmers \\uf164',\n",
    "    '704_Middle Class and Professional Groups': 'Professionals \\uf164',\n",
    "    '705_Underprivileged Minority Groups': 'Minority Groups \\uf164',\n",
    "    '706_Non-economic Demographic Groups': 'Demographic Groups \\uf164'\n",
    "}\n",
    "\n",
    "tags = pd.read_csv(os.path.join(RESULTS_DIR, 'tags_clean.csv'))\n",
    "tags\n",
    "\n",
    "# Ensure 'name-en' in answers and 'name' in tags are strings\n",
    "answers['name-en'] = answers['name-en'].astype(str)\n",
    "tags['name'] = tags['name'].astype(str)\n",
    "\n",
    "# Merge the DataFrames on the 'name' columns\n",
    "answers_tagged = answers.merge(tags, left_on='name-en', right_on='name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_models = pd.Series(index=answers_tagged['model'].unique(), data=np.ones(len(answers_tagged['model'].unique())))\n",
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from constants import model_abbreviations\n",
    "refusal_freq_dfs = []\n",
    "for tag_id, tag_name in cat_id_to_title.items():\n",
    "    tag_id_str = f\"categories.{tag_id}.result\"\n",
    "    answers_tagged_true = answers_tagged[answers_tagged[tag_id_str] == True]\n",
    "    answers_refusals = answers_tagged_true[answers_tagged_true['stage_1_response_valid'] == 'refusal']\n",
    "    answers_refusals = answers_refusals['model'].value_counts(normalize=True) * all_models\n",
    "    answers_refusals = answers_refusals.reset_index(name='count').rename(columns={'index': 'model'})\n",
    "    answers_refusals['tag'] = tag_name\n",
    "    refusal_freq_dfs.append(answers_refusals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat(refusal_freq_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "refusals_df = pd.concat(refusal_freq_dfs, ignore_index=True).fillna(0.0)\n",
    "refusals_df['model'] = refusals_df['model'].str.lower().map(model_abbreviations)\n",
    "# refusals_df['count'] = 1 - refusals_df['count']\n",
    "refusals_df = refusals_df.pivot(columns='tag', index='model')\n",
    "refusals_df = refusals_df.droplevel(None, axis=1)\n",
    "plt.figure(figsize=(5, 13))\n",
    "sns.heatmap(refusals_df.T, cbar_kws={\"shrink\": 0.5, 'label': 'Refusal Frequency'}, cmap='inferno_r')\n",
    "plt.gca().tick_params(top=False, bottom=False, labeltop=True, labelbottom=False)\n",
    "plt.gca().tick_params(axis='x', rotation=90)\n",
    "plt.xlabel(None)\n",
    "plt.ylabel(None)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'refusal_freq_heatmap.pdf'), bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers[answers['stage_1_response_valid'] == 'refusal']['model'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['model'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['topic_idx'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(answers['model'] + '-' + answers['language_code']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plot figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark invalid responses in the 'extracted' column\n",
    "answers.loc[answers['stage_1_response_valid'] != 'yes', 'extracted'] = 'invalid'\n",
    "answers.loc[answers['extracted'] == 'unknown', 'extracted'] = 'invalid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with one-hot encoding for the 'extracted' responses\n",
    "df_viz = pd.get_dummies(\n",
    "    answers[\n",
    "        [\n",
    "            \"question_idx\",\n",
    "            \"model\",\n",
    "            \"extracted\",\n",
    "            \"stage_1_response_valid\",\n",
    "            \"language_code\",\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"extracted\"],\n",
    "    prefix=\"response\",\n",
    "    prefix_sep=\"_\",\n",
    ")\n",
    "\n",
    "# Define the list of valid responses\n",
    "valid_responses = [\n",
    "    \"very negative\",\n",
    "    \"negative\",\n",
    "    \"neutral\",\n",
    "    \"positive\",\n",
    "    \"very positive\",\n",
    "    \"invalid\",\n",
    "]\n",
    "\n",
    "# Define colors for each response type\n",
    "response_colors = {\n",
    "    \"very negative\": \"chocolate\",\n",
    "    \"negative\": \"darkorange\",\n",
    "    \"neutral\": \"gray\",\n",
    "    \"positive\": \"limegreen\",\n",
    "    \"very positive\": \"darkgreen\",\n",
    "    \"invalid\": \"black\",\n",
    "}\n",
    "\n",
    "# Display the prepared DataFrame\n",
    "df_viz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df_viz['model'] + '-' + df_viz['language_code']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import model_abbreviations\n",
    "\n",
    "# Set font size for plots\n",
    "FONTSIZE = 12\n",
    "plt.rcParams.update({\"font.size\": FONTSIZE})\n",
    "plt.rc(\"font\", size=FONTSIZE)  # Controls default text sizes\n",
    "\n",
    "# Set parameters for visualization\n",
    "show_percentages = True\n",
    "df_viz['model_abbr'] = df_viz['model'].str.lower().map(model_abbreviations)\n",
    "groupby_colname = [\"model_abbr\", \"language_code\"]\n",
    "\n",
    "# Aggregate the data by model and compute the mean for each response type\n",
    "df_viz_agg = df_viz.groupby(groupby_colname).agg(\n",
    "    **{f\"{k}\": (f\"response_{k}\", \"mean\") for k in valid_responses}\n",
    ").reset_index()\n",
    "\n",
    "# Sort the DataFrame for consistent plotting\n",
    "df_viz_agg = df_viz_agg.sort_values(['language_code', 'model_abbr'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_viz_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language, df_language in df_viz_agg.groupby('language_code'):\n",
    "    # Drop unnecessary columns and set 'model' as the index\n",
    "    df_language = df_language.drop(columns=['language_code'])\n",
    "    df_language = df_language.set_index('model_abbr')\n",
    "\n",
    "    # Compute the percentage of invalid responses\n",
    "    fraction_invalid = df_language['invalid'] * 100\n",
    "\n",
    "    # # Limit the number of models displayed if necessary\n",
    "    # if len(df_language) > 100:\n",
    "    #     show_every = len(df_language) // 60\n",
    "    #     df_language = df_language.iloc[::show_every]\n",
    "    #     fraction_invalid = fraction_invalid.iloc[::show_every]\n",
    "\n",
    "    # Create subplots with shared Y-axis\n",
    "    v_size = 1 + len(df_language) * 0.7\n",
    "    fig, (ax, ax2) = plt.subplots(\n",
    "        nrows=1,\n",
    "        ncols=2,\n",
    "        sharey=True,\n",
    "        width_ratios=[4, 1],\n",
    "        figsize=(15, v_size),\n",
    "        gridspec_kw={'wspace': 0.2},\n",
    "    )\n",
    "\n",
    "    # Prepare data for the stacked bar chart (excluding 'invalid')\n",
    "    df_responses = df_language[[col for col in df_language.columns if col != 'invalid']]\n",
    "    df_responses = df_responses.div(df_responses.sum(axis=1), axis=0)\n",
    "    df_responses.sort_values('model_abbr', ascending=True)\n",
    "\n",
    "    # Plot the stacked horizontal bar chart for valid responses\n",
    "    df_responses.plot(\n",
    "        kind='barh',\n",
    "        stacked=True,\n",
    "        color=[response_colors[k] for k in valid_responses if k != 'invalid'],\n",
    "        alpha=0.7,\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "        fontsize=FONTSIZE,\n",
    "    )\n",
    "\n",
    "    # Set Y-axis labels using model abbreviations\n",
    "    ax.set_yticks(range(len(df_responses)))\n",
    "    yticklabels = [\n",
    "        model_abbreviations.get('-'.join(label.split('-')[:-1]), label)\n",
    "        for label in df_responses.index\n",
    "    ]\n",
    "    ax.set_yticklabels(yticklabels, fontsize=FONTSIZE)\n",
    "\n",
    "    # Plot the bar chart for invalid responses\n",
    "    fraction_invalid.plot(\n",
    "        kind='barh',\n",
    "        color=response_colors['invalid'],\n",
    "        alpha=0.7,\n",
    "        ax=ax2,\n",
    "        legend=False,\n",
    "        fontsize=FONTSIZE,\n",
    "        edgecolor='none'\n",
    "    )\n",
    "\n",
    "    # Sync Y-axis labels on the second plot\n",
    "    ax2.set_yticks(range(len(df_responses)))\n",
    "    ax2.set_yticklabels(yticklabels, fontsize=FONTSIZE)\n",
    "\n",
    "    # Set X-axis limits and labels for the invalid responses plot\n",
    "    ax2.set_xlim(0, 100)\n",
    "    ax2.set_xticks([0, 100])\n",
    "    ax2.set_xticklabels(['0', '100%'], fontsize=FONTSIZE)\n",
    "\n",
    "    # Set X-axis ticks and labels for the valid responses plot\n",
    "    xticks = np.arange(0, 1.1, 0.5)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([f\"{int(x * 100)}%\" for x in xticks], fontsize=FONTSIZE)\n",
    "\n",
    "    # Annotate percentages on the bars if enabled\n",
    "    if show_percentages:\n",
    "        for i in range(len(df_responses)):\n",
    "            row = df_responses.iloc[i]\n",
    "            sum_so_far = 0\n",
    "            for col in row.index:\n",
    "                value = row[col]\n",
    "                if value > 0:\n",
    "                    percentage = int(value * 100)\n",
    "                    if percentage > 0:\n",
    "                        ax.text(\n",
    "                            sum_so_far + value / 2,\n",
    "                            i,\n",
    "                            f\"{percentage}%\",\n",
    "                            ha='center',\n",
    "                            va='center',\n",
    "                            color='white',\n",
    "                            fontsize=FONTSIZE - 2,\n",
    "                        )\n",
    "                    sum_so_far += value\n",
    "            invalid_percentage = fraction_invalid.iloc[i]\n",
    "            if invalid_percentage > 0:\n",
    "                ax2.text(\n",
    "                    invalid_percentage,\n",
    "                    i,\n",
    "                    f\"{invalid_percentage:.1f}%\" if invalid_percentage > 1 else f\"{invalid_percentage:.1g}%\",\n",
    "                    ha='left',\n",
    "                    va='center',\n",
    "                    color='gray',\n",
    "                    fontsize=FONTSIZE - 2,\n",
    "                )\n",
    "\n",
    "    # Combine and customize the legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(\n",
    "        handles,\n",
    "        [label.capitalize() for label in df_responses.columns],\n",
    "        loc='upper center',  # Position the legend at the top center\n",
    "        bbox_to_anchor=(0.5, 1.1),  # Move the legend above the plot\n",
    "        ncol=5,  # Number of columns for the legend\n",
    "        fontsize=FONTSIZE,\n",
    "        facecolor='white',\n",
    "    )\n",
    "\n",
    "    # Set labels and nothing for y-axis label\n",
    "    ax.set_xlabel('Label distribution among valid responses')\n",
    "    ax2.set_xlabel('% of responses that were invalid')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    # Remove spines and adjust layout\n",
    "    sns.despine(fig=fig, ax=[ax, ax2], top=True, right=True, left=True, bottom=False)\n",
    "    plt.subplots_adjust(wspace=0.05)\n",
    "\n",
    "    ax.grid(False)\n",
    "    ax2.grid(False)\n",
    "\n",
    "    # Save the figure\n",
    "    fname = f\"response_distribution_{language}.pdf\"\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, fname), bbox_inches='tight', transparent=True)\n",
    "    print(f\"Saved figure to {os.path.join(FIGURES_DIR, fname)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out invalid responses based on stage 1 validation\n",
    "stage_1_validation = True\n",
    "if stage_1_validation:\n",
    "    nb_answers = len(answers)\n",
    "    answers = answers[answers['stage_1_response_valid'] == 'yes']\n",
    "    print(f\"Filtered out {(nb_answers - len(answers)) * 100 / nb_answers:.2f}% out of {nb_answers} answers due to poor stage 1 responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out answers with unknown scores due to poor Stage 3 responses\n",
    "nb_answers = len(answers)\n",
    "answers = answers[answers['score'] != 'unknown']\n",
    "filtered_percentage = (nb_answers - len(answers)) * 100 / nb_answers\n",
    "print(f\"Filtered out {filtered_percentage:.6f}% of {nb_answers} answers due to poor Stage 3 responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['model_lang'] = answers['model'] + '-' + answers['language_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of answers per model\n",
    "freq_per_model = answers.groupby('model_lang')['question_idx'].count().rename('freq')\n",
    "\n",
    "# Set the minimum number of answers required per model\n",
    "min_model_freq = answers['topic_idx'].nunique() // 6\n",
    "\n",
    "# Plot the number of answers per model\n",
    "plt.figure(figsize=(12, 13))\n",
    "sns.barplot(\n",
    "    x=freq_per_model.sort_values(ascending=True).index,\n",
    "    y=freq_per_model.sort_values(ascending=True).values,\n",
    "    order=freq_per_model.sort_values(ascending=True).index\n",
    ")\n",
    "plt.axhline(y=min_model_freq, color='red', linestyle='--', label='Minimum Required Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Number of Answers')\n",
    "plt.title('Number of Answers per Model')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out models with less than the minimum required answers\n",
    "nb_models = answers['model_lang'].nunique()\n",
    "models_to_keep = freq_per_model[freq_per_model >= min_model_freq].index\n",
    "answers = answers[answers['model_lang'].isin(models_to_keep)]\n",
    "filtered_models = nb_models - answers['model_lang'].nunique()\n",
    "print(f\"Filtered out {filtered_models} out of {nb_models} models with less than {min_model_freq} answers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the below by language\n",
    "min_question_freqs = {}\n",
    "for lang, lang_df in answers.groupby('language_code'):\n",
    "    num_models = lang_df['model'].nunique()\n",
    "    min_question_freq = int(np.ceil(num_models / 2))\n",
    "    freq_per_question = lang_df.groupby('question_idx')['model'].count().rename('freq')\n",
    "    min_question_freqs[lang] = min_question_freq\n",
    "    \n",
    "    # Plot a histogram of the frequency of models per question\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.histplot(\n",
    "        freq_per_question,\n",
    "        bins=range(0, num_models + 2),\n",
    "        discrete=True,\n",
    "        binrange=(0, num_models)\n",
    "    )\n",
    "    \n",
    "    plt.xticks(range(0, num_models + 1))\n",
    "    plt.axvline(x=min_question_freq, color='red', linestyle='--', label='Minimum Required Frequency')\n",
    "    plt.xlabel('Number of Models that Answered the Question')\n",
    "    plt.ylabel('Number of Questions')\n",
    "    plt.title(f'Distribution of Model Frequencies per Question ({lang})')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # Calculate the number of unique original models\n",
    "# num_model_original = answers['model'].nunique()\n",
    "# # Set the minimum number of models required per question\n",
    "# # min_question_freq = 8  # Alternatively, use int(np.ceil(num_model_original / 2))\n",
    "# min_question_freq = int(np.ceil(num_model_original / 2))\n",
    "# # Calculate the frequency of models per question\n",
    "# freq_per_question = answers.groupby('topic_idx')['model'].count().rename('freq')\n",
    "# \n",
    "# # Plot a histogram of the frequency of models per question\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.histplot(\n",
    "#     freq_per_question,\n",
    "#     bins=range(0, num_model_original + 2),\n",
    "#     discrete=True,\n",
    "#     binrange=(0, num_model_original)\n",
    "# )\n",
    "# plt.xticks(range(0, num_model_original + 1))\n",
    "# plt.axvline(x=min_question_freq, color='red', linestyle='--', label='Minimum Required Frequency')\n",
    "# plt.xlabel('Number of Models that Answered the Question')\n",
    "# plt.ylabel('Number of Questions')\n",
    "# plt.title('Distribution of Model Frequencies per Question')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filtered_total = 0\n",
    "nb_questions_total = answers['question_idx'].nunique()\n",
    "for lang, min_freq in min_question_freqs.items():\n",
    "    nb_questions = answers[answers['language_code'] == lang]['question_idx'].nunique()\n",
    "    freq_per_question = answers[answers['language_code'] == lang].groupby('question_idx')['model'].count().rename('freq')\n",
    "    questions_to_drop = freq_per_question[freq_per_question < min_freq].index\n",
    "    answers = answers[~answers['question_idx'].isin(questions_to_drop)]\n",
    "    filtered_questions = nb_questions - answers[answers['language_code'] == lang]['question_idx'].nunique()\n",
    "    filtered_percentage = (filtered_questions) * 100 / nb_questions\n",
    "    print(f\"Filtered out {filtered_percentage:.2f}% of {nb_questions} questions with less than {min_freq} models ({lang}).\")\n",
    "    num_filtered_total += filtered_questions\n",
    "filtered_percentage = (num_filtered_total) * 100 / nb_questions_total\n",
    "print(f\"Filtered out {filtered_percentage} questions in total.\")\n",
    "    \n",
    "# Filter out questions answered by fewer than the minimum required number of models\n",
    "# nb_questions = answers['question_idx'].nunique()\n",
    "# questions_to_keep = freq_per_question[freq_per_question >= min_question_freq].index\n",
    "# answers = answers[answers['question_idx'].isin(questions_to_keep)]\n",
    "# filtered_questions = nb_questions - answers['question_idx'].nunique()\n",
    "# filtered_percentage = (filtered_questions) * 100 / nb_questions\n",
    "# print(f\"Filtered out {filtered_percentage:.2f}% of {nb_questions} questions with less than {min_question_freq} models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"At the end, {len(answers)} answers remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many answers, models, and topics are in this dataframe?\n",
    "print(\n",
    "    f\"answers: {len(answers)}\\n\"\n",
    "    + f\"models (en and zh): {len(answers.model.unique())}, \\n\"\n",
    "    + f\"topics (en and zh): {len(answers.topic.unique())}, topics_idx: {len(answers.topic_idx.unique())}\\n\"\n",
    "    + f\"question_idx: {len(answers.question_idx.unique())}\\n\"\n",
    "    f\"prompt_template_idx (0 is English, 1 Chinese): {len(answers.prompt_template_idx.unique())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang_pct = answers.groupby('language')['extracted'].value_counts(normalize=True)\n",
    "lang_pct = lang_pct.unstack()\n",
    "lang_pct.index = lang_pct.index.str.replace(' (Simplified)', '')\n",
    "lang_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "lang_pct.plot(\n",
    "    kind='barh',\n",
    "    stacked=True,\n",
    "    color=[response_colors[k] for k in valid_responses if k != 'invalid'],\n",
    "    alpha=0.7,\n",
    "    legend=False,\n",
    "    fontsize=FONTSIZE,\n",
    "    ax=ax\n",
    ")\n",
    "lang_score_mean = answers.groupby('language')['score'].mean()\n",
    "ax.vlines(lang_score_mean, np.arange(len(lang_score_mean))-0.5, np.arange(len(lang_score_mean))+0.5, color='red', linestyle='--', label=None)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(\n",
    "    handles,\n",
    "    [label.capitalize() for label in df_responses.columns],\n",
    "    loc='upper center',  # Position the legend at the top center\n",
    "    bbox_to_anchor=(0.5, 1.1),  # Move the legend above the plot\n",
    "    ncol=5,  # Number of columns for the legend\n",
    "    fontsize=FONTSIZE,\n",
    "    facecolor='white',\n",
    ")\n",
    "ax.set_xlabel('Label distribution among valid responses')\n",
    "ax.set_ylabel(None)\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'response_distribution_lang.pdf'), bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers.groupby('name-en')['score'].mean().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop summaries first\n",
    "answers = answers.drop(columns=[col for col in answers.columns if col.startswith('summary-')] + ['Unnamed: 0', 'model_lang'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory to save the processed data\n",
    "# FIGURES_DIR = os.path.join(os.path.abspath(''), '..', 'data', 'processed')\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Construct the full file path for the output CSV file\n",
    "output_file = os.path.join(FIGURES_DIR, 'answers_processed.csv')\n",
    "\n",
    "# Save the processed 'answers' DataFrame to a CSV file without the index\n",
    "answers.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processed 'answers' DataFrame saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
