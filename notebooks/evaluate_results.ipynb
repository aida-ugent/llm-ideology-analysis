{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Results\n",
    "## Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "import os\n",
    "from matplotlib import rcParams, font_manager\n",
    "import matplotlib.font_manager as fm\n",
    "from pyfonts import load_font\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import combinations\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get paths from environment with defaults\n",
    "RESULTS_DIR = os.getenv('RESULTS_DIR')\n",
    "RESULTS_ALL_BEST_DIR = os.getenv('RESULTS_ALL_BEST_DIR')\n",
    "FIGURES_DIR = os.getenv('FIGURES_DIR')\n",
    "\n",
    "# Define the directories\n",
    "ROOT_DIR = os.path.join(os.path.abspath(\"\"), \"..\")\n",
    "# PROCESSED_ANSWERS_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "PROCESSED_ANSWERS_DIR = RESULTS_ALL_BEST_DIR\n",
    "# PROCESSED_ANSWERS_DIR = os.path.join(ROOT_DIR, 'results', 'all_best_v2_un')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up color maps with NaN handling\n",
    "coolwarm_nan = colormaps[\"coolwarm\"]\n",
    "coolwarm_nan.set_bad(\"gray\")\n",
    "\n",
    "viridis_nan = colormaps[\"viridis\"]\n",
    "viridis_nan.set_bad(\"gray\")\n",
    "\n",
    "# Configure seaborn theme\n",
    "sns.set_theme(context=\"notebook\", style=\"white\", font=\"sans-serif\", font_scale=0.9)\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option(\"display.max_colwidth\", 90)\n",
    "pd.set_option(\"display.width\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# Load non solid fonts\n",
    "non_solid_font_path = f'{FIGURES_DIR}/fonts/FontAwesome6NonSolid.otf'\n",
    "non_solid_font_properties = FontProperties(fname=non_solid_font_path)\n",
    "fm.fontManager.addfont(non_solid_font_properties.get_file())\n",
    "\n",
    "# Load solid fonts\n",
    "solid_font_path = f'{FIGURES_DIR}/fonts/FontAwesome6Solid.otf'\n",
    "solid_font_properties = FontProperties(fname=solid_font_path)\n",
    "fm.fontManager.addfont(solid_font_properties.get_file())\n",
    "\n",
    "# Rename font properties\n",
    "# fm.fontManager.ttflist[-2].name = 'Non Solid FA'\n",
    "# fm.fontManager.ttflist[-1].name = 'Solid FA'\n",
    "\n",
    "# Load font family params fonts\n",
    "# plt.rcParams['font.family'] = ['DejaVu Sans', 'Non Solid FA','Solid FA']\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'Font Awesome 6 Free Regular', 'Font Awesome 6 Free']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Load the Symbola font\n",
    "# font = load_font(\n",
    "#     font_url=\"https://github.com/ChiefMikeK/ttf-symbola/blob/master/Symbola.ttf?raw=true\"\n",
    "# )\n",
    "# \n",
    "# fm.fontManager.addfont(font.get_file())\n",
    "# \n",
    "# # Verify that the fonts have been added\n",
    "# font_list = [f.name for f in fm.fontManager.ttflist]\n",
    "# print(\"DejaVu Sans\" in font_list)        # Should return True\n",
    "# print(\"Symbola\" in font_list)   # Should return True\n",
    "# \n",
    "# # 'Segoe UI Emoji' is a proprietary font provided by Microsoft and is only included with Windows operating systems.\n",
    "# plt.rcParams['font.family'] = ['DejaVu Sans', 'Symbola']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed answers\n",
    "answers = pd.read_csv(os.path.join(PROCESSED_ANSWERS_DIR, 'answers_processed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers['model_original'] = answers['model']\n",
    "answers['model'] = answers['model'] + '-' + answers['language_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many answers, models, and topics are in this dataframe?\n",
    "# print(\n",
    "#     f\"answers: {len(answers)}\\n\"\n",
    "#     + f\"models (en and zh): {len(answers.model.unique())}, models_original: {len(answers.model_original.unique())}\\n\"\n",
    "#     + f\"topics (en and zh): {len(answers.topic.unique())}, topics_idx: {len(answers.topic_idx.unique())}\\n\"\n",
    "#     + f\"question_idx: {len(answers.question_idx.unique())}\\n\"\n",
    "#     f\"prompt_template_idx (0 is English, 1 Chinese): {len(answers.prompt_template_idx.unique())}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Person Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already added in preprocessing...\n",
    "# # Add person descriptions\n",
    "# person_descriptions_path = os.path.join(ROOT_DIR, 'docs', 'topics', 'v2.0_people_summaries.csv')\n",
    "# person_descriptions = pd.read_csv(person_descriptions_path)\n",
    "# \n",
    "# # Merge descriptions into answers DataFrame\n",
    "# answers = answers.merge(person_descriptions, on='name-en', how='left', suffixes=('', '_extra'))\n",
    "# \n",
    "# # Drop any additional columns that may have been added during the merge\n",
    "# answers = answers.drop(columns=[col for col in answers.columns if col.endswith('_extra')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Mean Scores Between Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean scores for each language\n",
    "mean_scores = answers.groupby('language')['score'].mean()\n",
    "\n",
    "# Compute differences between each pair of languages\n",
    "# Get all unique pairs of languages\n",
    "language_pairs = combinations(mean_scores.index, 2)\n",
    "\n",
    "# Create a list to hold the differences\n",
    "diffs = []\n",
    "\n",
    "# Calculate the difference for each pair\n",
    "for lang1, lang2 in language_pairs:\n",
    "    diff = mean_scores[lang1] - mean_scores[lang2]\n",
    "    diffs.append({\n",
    "        'Language 1': lang1,\n",
    "        'Language 2': lang2,\n",
    "        'Mean Score Difference': diff\n",
    "    })\n",
    "\n",
    "# Step 3: Convert the list to a DataFrame\n",
    "diffs_df = pd.DataFrame(diffs)\n",
    "\n",
    "# Display the differences\n",
    "print(diffs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Scores per Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average scores per person\n",
    "average_scores_per_person = answers.groupby('name-en')['score'].mean().sort_values(ascending=True)\n",
    "average_scores_per_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Answers per Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers per question index\n",
    "question_counts = (\n",
    "    answers.groupby(\"question_idx\")\n",
    "    .count()[\"model\"]\n",
    "    .rename(\"number of answers per question_idx\")\n",
    ")\n",
    "\n",
    "# Plot the distribution\n",
    "sns.histplot(question_counts, discrete=True)\n",
    "plt.xlabel('Number of Answers per Question Index')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Answers per Question Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers per model\n",
    "model_counts = answers.groupby('model').count()['question_idx'].rename('count')\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Plot the counts per model\n",
    "sns.barplot(\n",
    "    x=model_counts.index,\n",
    "    y=model_counts,\n",
    "    order=model_counts.sort_values(ascending=False).index\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Number of Answers')\n",
    "plt.title('Number of Answers per Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Specific responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topic_to_test = 'Shah Rukh Khan'\n",
    "# all_models = answers['model'].unique()\n",
    "# model_to_test = [model for model in all_models if model.endswith('-fr')]\n",
    "# hits = answers[(answers['short_name'] == topic_to_test) & (answers['model'].isin(model_to_test))]\n",
    "# for i, hit in hits.iterrows():\n",
    "#     print(hit['model'])\n",
    "#     print(hit['stage_1_response'])\n",
    "#     print(hit['stage_1_response_valid'])\n",
    "#     print(hit['stage_3_response'])\n",
    "#     print(hit['extracted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Model Subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = model_counts.index.values\n",
    "\n",
    "model_groups = {}\n",
    "\n",
    "########################################################\n",
    "############### Language blocks ########################\n",
    "########################################################\n",
    "\n",
    "def get_language(model_name):\n",
    "    if '-' in model_name:\n",
    "        return model_name.rsplit('-', 1)[-1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "languages = set(get_language(model) for model in all_models)\n",
    "\n",
    "# Group models by language\n",
    "language_models = {lang: [] for lang in languages}\n",
    "for model in all_models:\n",
    "    lang = get_language(model)\n",
    "    if lang in languages:\n",
    "        language_models[lang].append(model)\n",
    "\n",
    "# Add language groupings\n",
    "model_groups |= language_models\n",
    "\n",
    "# \"All-but-language\" groups\n",
    "for lang in languages:\n",
    "    model_groups['not-' + lang] = [model for model in all_models \n",
    "                                   if model not in model_groups[lang]]\n",
    "    \n",
    "########################################################\n",
    "############# Geopolitical blocks ######################\n",
    "########################################################\n",
    "\n",
    "block_alignment = {'united-states': ['anthropic', 'gemini',\n",
    "                                     'openai', 'llama', 'grok'],\n",
    "                   \"chinese\": ['ernie', 'qwen', 'deepseek', \n",
    "                               'baichuan'],\n",
    "                   \"russian\": ['gigachat', 'vikhr', 'yandex'],\n",
    "                   'israeli': ['jamba'],\n",
    "                   'arabic': ['jais', 'silma'],\n",
    "                   'european': ['mistral', 'teuken']}\n",
    "\n",
    "for block, block_companies in block_alignment.items():\n",
    "    model_groups[block] = [model for model in all_models \n",
    "                           if any(company in model.lower() \n",
    "                                  for company in block_companies)]\n",
    "    model_groups['not-' + block] = [model for model in all_models \n",
    "                                    if model not in model_groups[block]]\n",
    "\n",
    "for block in block_alignment.keys():\n",
    "    # Restrain block comparison to languages \n",
    "    # supported by at least one LLM on both side\n",
    "    model_groups[block + \"-shared\"] = \\\n",
    "        [model for model in model_groups[block]\n",
    "         if get_language(model) in set([get_language(model) \n",
    "         for model in model_groups['not-' + block]])]\n",
    "    model_groups['not-' + block + \"-shared\"] = \\\n",
    "        [model for model in model_groups['not-' + block]\n",
    "         if get_language(model) in set([get_language(model) \n",
    "         for model in model_groups[block]])]\n",
    "    # Restrictions to languages supported by at least one LLM\n",
    "    # on both side when comparing to US models\n",
    "    if block != 'united-states':\n",
    "        model_groups[block + \"-shared-us\"] = \\\n",
    "            [model for model in model_groups[block]\n",
    "             if get_language(model) in set([get_language(model) \n",
    "             for model in model_groups['united-states']])]\n",
    "        model_groups['not-' + block + \"-shared-us\"] = \\\n",
    "            [model for model in model_groups['united-states']\n",
    "             if get_language(model) in set([get_language(model) \n",
    "             for model in model_groups[block + \"-shared-us\"]])]\n",
    "\n",
    "# For blocks only: harmonize wrt language\n",
    "\n",
    "########################################################\n",
    "############# Within block analysis ####################\n",
    "########################################################\n",
    "\n",
    "\n",
    "model_groups['arabic-ar'] = [model for model in model_groups['arabic']\n",
    "                             if get_language(model) == 'ar']\n",
    "model_groups['israeli-en'] = [model for model in model_groups['israeli']\n",
    "                             if get_language(model) == 'en']\n",
    "model_groups['russian-ru'] = [model for model in model_groups['russian']\n",
    "                              if get_language(model) == 'ru']\n",
    "model_groups['united-states-en'] = [model for model in model_groups['united-states']\n",
    "                                    if get_language(model) == 'en']\n",
    "model_groups['european-en-fr-es'] = [model for model in model_groups['european']\n",
    "                                     if get_language(model) in ['en', 'fr', 'es']]\n",
    "\n",
    "\n",
    "\n",
    "# US LLMs in English\n",
    "for company in block_alignment['united-states']:\n",
    "    model_groups[company + '-en'] = [model for model in all_models \n",
    "                                     if company in model.lower()\n",
    "                                     and get_language(model) == 'en']\n",
    "    model_groups['united-states-en-not-' + company] = \\\n",
    "        list(set(model_groups['united-states-en']).difference(model_groups[company + '-en']))\n",
    "\n",
    "# Chinese LLMs in Chinese\n",
    "model_groups['chinese-zh'] = [model for model in model_groups['chinese']\n",
    "                              if get_language(model) == 'zh']    \n",
    "for company in block_alignment['chinese']:\n",
    "    model_groups[company+'-zh'] = [model for model in all_models \n",
    "                             if company in model.lower()\n",
    "                             and get_language(model) == 'zh']\n",
    "    model_groups['chinese-zh-not-' + company] = \\\n",
    "        list(set(model_groups['chinese-zh']).difference(model_groups[company + '-zh']))\n",
    "\n",
    "\n",
    "blocks_self_language = ['chinese-zh', 'arabic-ar', 'israeli-en',\n",
    "                        'russian-ru', 'united-states-en',\n",
    "                        'european-en-fr-es']\n",
    "for country_language in blocks_self_language:\n",
    "    for other_country_language in blocks_self_language:\n",
    "        if other_country_language != country_language:\n",
    "            model_groups['not-' + country_language] = \\\n",
    "                model_groups.get('not-' + country_language, []) \\\n",
    "                + model_groups[other_country_language]\n",
    "\n",
    "# Define group names for labeling\n",
    "model_group_names = {\n",
    "    \"fr\": \"Respondents in French\",\n",
    "    \"es\": \"Respondents in Spanish\",\n",
    "    \"ru\": \"Respondents in Russian\",\n",
    "    \"zh\": \"Respondents in Chinese\",\n",
    "    \"ar\": \"Respondents in Arabic\",\n",
    "    \"en\": \"Respondents in English\",\n",
    "\n",
    "    \"not-fr\": \"Respondents excluding ones in French\",\n",
    "    \"not-es\": \"Respondents excluding ones in Spanish\",\n",
    "    \"not-ru\": \"Respondents excluding ones in Russian\",\n",
    "    \"not-zh\": \"Respondents excluding ones in Chinese\",\n",
    "    \"not-ar\": \"Respondents excluding ones in Arabic\",\n",
    "    \"not-en\": \"Respondents excluding ones in English\",\n",
    "\n",
    "    \"chinese-shared\": \"LLMs by Chinese companies\",\n",
    "    \"russian-shared\": \"LLMs by Russian companies\",\n",
    "    \"united-states-shared\": \"LLMs by United States companies\",\n",
    "    \"european-shared\": \"LLMs by EU companies\",\n",
    "    \"arabic-shared\": \"LLMs by Arabic companies\",\n",
    "    \"israeli-shared\": \"LLMs by Israeli companies\",\n",
    "\n",
    "    \"not-chinese-shared\": \"LLMs excluding Chinese companies\",\n",
    "    \"not-russian-shared\": \"LLMs excluding Russian companies\",\n",
    "    \"not-united-states-shared\": \"LLMs excluding United States companies\",\n",
    "    \"not-european-shared\": \"LLMs excluding EU companies\",\n",
    "    \"not-arabic-shared\": \"LLMs excluding Arabic companies\",\n",
    "    \"not-israeli-shared\": \"LLMs excluding Israeli companies\",\n",
    "\n",
    "    \"chinese-shared-us\": \"LLMs by Chinese companies\",\n",
    "    \"russian-shared-us\": \"LLMs by Russian companies\",\n",
    "    \"united-states-shared-us\": \"LLMs by United States companies\",\n",
    "    \"european-shared-us\": \"LLMs by EU companies\",\n",
    "    \"arabic-shared-us\": \"LLMs by Arabic companies\",\n",
    "    \"israeli-shared-us\": \"LLMs by Israeli companies\",\n",
    "\n",
    "\n",
    "    \"not-chinese-shared-us\": \"LLMs by United States companies\",\n",
    "    \"not-russian-shared-us\": \"LLMs by United States companies\",\n",
    "    \"not-european-shared-us\": \"LLMs by United States companies\",\n",
    "    \"not-arabic-shared-us\": \"LLMs by United States companies\",\n",
    "    \"not-israeli-shared-us\": \"LLMs by United States companies\",\n",
    "\n",
    "    'chinese-zh': \"LLMs by Chinese companies (in Chinese)\",\n",
    "    \"arabic-ar\": \"LLMs by Arabic companies in Arabic\",\n",
    "    \"israeli-en\": \"LLMs by Israeli companies in English\",\n",
    "    \"russian-ru\": \"LLMs by Russian companies in Russian\",\n",
    "    \"united-states-en\": \"LLMs by United States companies (in English)\",\n",
    "    \"european-en-fr-es\": \"LLMs by European companies in English, French or Spanish\", \n",
    "    'not-chinese-zh': \"LLMs of non-Chinese companies in their usual prompting language\",\n",
    "    'not-arabic-ar': \"LLMs of non-Arabic companies in their usual prompting language\",\n",
    "    'not-israeli-en': \"LLMs of non-Israeli companies in their usual prompting language\",\n",
    "    'not-united-states-en': \"LLMs of non-US companies in their usual prompting language\",\n",
    "    'not-european-en-fr-es': \"LLMs of non-European companies in their usual prompting language\",\n",
    "    'not-russian-ru': \"LLMs of non-Russian companies in their usual prompting language\",\n",
    "\n",
    "    \"ernie-zh\": \"Wenxiaoyan (Baidu) in Chinese\",\n",
    "    \"chinese-zh-not-ernie\": \"Chinese LLMs (in Chinese) except Wenxiaoyan\",\n",
    "    \"qwen-zh\": \"Qwen (Alibaba) LLMs in Chinese\",\n",
    "    \"chinese-zh-not-qwen\": \"Chinese LLMs (in Chinese) except Qwen\",\n",
    "    \"baichuan-zh\": \"Baichuan LLMs in Chinese\",\n",
    "    \"chinese-zh-not-baichuan\": \"Chinese LLMs (in Chinese) except Baichuan\",\n",
    "    \"deepseek-zh\": \"Deepseek LLMs in Chinese\",\n",
    "    \"chinese-zh-not-deepseek\": \"Chinese LLMs (in Chinese) except Deepseek\",\n",
    "\n",
    "    \"anthropic-en\": \"Claude (Anthropic) in English\",\n",
    "    \"llama-en\": \"LLaMa (Meta) in English\",\n",
    "    \"gemini-en\": \"Gemini (Google) in English\",\n",
    "    \"mistral-en\": \"Mistral in English\",\n",
    "    \"openai-en\": \"GPT-4o (OpenAI) in English\",\n",
    "    \"grok-en\": \"Grok (xAI) in English\",\n",
    "    \"united-states-en-not-anthropic\": \"US LLMs (in English) except Claude\",\n",
    "    \"united-states-en-not-llama\": \"US LLMs (in English) except LLaMa\",\n",
    "    \"united-states-en-not-gemini\": \"US LLMs (in English) except Gemini\",\n",
    "    \"united-states-en-not-mistral\": \"US LLMs (in English) except Mistral\",\n",
    "    \"united-states-en-not-openai\": \"US LLMs (in English) except GPT-4o\",\n",
    "    \"united-states-en-not-grok\": \"US LLMs (in English) except Grok\",\n",
    "}\n",
    "\n",
    "ova_languages = [(lang, 'not-' + lang) for lang in languages]\n",
    "ove_languages = [(lang, 'en') for lang in languages if lang != 'en']\n",
    "ova_blocks = [(block + \"-shared\", 'not-' + block + \"-shared\") \n",
    "              for block in block_alignment.keys()]\n",
    "ove_blocks = [(block + \"-shared-us\", 'not-' + block + \"-shared-us\") \n",
    "              for block in block_alignment.keys()\n",
    "              if block != 'united-states']\n",
    "ova_blocks_self_lang = [(block_lang, 'not-' + block_lang)\n",
    "                        for block_lang in ['chinese-zh', \n",
    "                                           'arabic-ar', 'israeli-en',\n",
    "                                           'russian-ru', \n",
    "                                           'united-states-en',\n",
    "                                           'european-en-fr-es']]\n",
    "ova_western = [(company + \"-en\", 'united-states-en-not-' + company)\n",
    "               for company in block_alignment['united-states']]\n",
    "ova_chinese = [(\"qwen-zh\", \"chinese-zh-not-qwen\"), \n",
    "               (\"ernie-zh\", \"chinese-zh-not-ernie\"),\n",
    "               (\"baichuan-zh\", \"chinese-zh-not-baichuan\"),\n",
    "               (\"deepseek-zh\", \"chinese-zh-not-deepseek\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function to Parse Model Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_model_group(model_group_str):\n",
    "    if isinstance(model_group_str, list):\n",
    "        return model_group_str\n",
    "    if model_group_str in model_groups:\n",
    "        return model_groups[model_group_str]\n",
    "    if model_group_str in all_models:\n",
    "        return [model_group_str]\n",
    "    raise ValueError(f\"Model group {model_group_str} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Comparison\n",
    "### Creating Short Names for Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_short_name(row):\n",
    "    wikidata_len = len(row[\"name-en\"])\n",
    "    wikititle_len = len(row[\"wiki_title-en\"])\n",
    "    if wikidata_len < wikititle_len:\n",
    "        short_name = row[\"name-en\"]\n",
    "    else:\n",
    "        short_name = row[\"wiki_title-en\"]\n",
    "    if len(short_name) > 20:\n",
    "        if ',' in short_name:\n",
    "            short_name = short_name.split(',')[0] + f' ({short_name.split(\" \")[-1]})'\n",
    "        elif ' of ' in short_name:\n",
    "            short_name = short_name.split(' of')[0] + f' ({short_name.split(\" \")[-1]})'\n",
    "        elif ' von ' in short_name:\n",
    "            short_name = short_name.split(' von')[0] + f' ({short_name.split(\" \")[-1]})'\n",
    "    return short_name\n",
    "\n",
    "answers[\"short_name\"] = answers.apply(get_short_name, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Comparison\n",
    "### Function to Compare Scores Between Model Groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scores(df, model_group_1_str, model_group_2_str, score_col='score', n_resamples=10000, ci_alpha=0.05):\n",
    "    model_group_1 = parse_model_group(model_group_1_str)\n",
    "    model_group_2 = parse_model_group(model_group_2_str)\n",
    "\n",
    "    df_grouped_1 = df[df.model.isin(model_group_1)].groupby(\"short_name\")\n",
    "    df_grouped_2 = df[df.model.isin(model_group_2)].groupby(\"short_name\")\n",
    "    common_topics = df_grouped_1.groups.keys() & df_grouped_2.groups.keys()\n",
    "    \n",
    "    overall_mean_1 = np.mean([df_grouped_1.get_group(topic)[score_col].mean() for topic in common_topics])\n",
    "    print(f\"Overall mean group {model_group_1_str}: {overall_mean_1}\")\n",
    "    overall_mean_2 = np.mean([df_grouped_2.get_group(topic)[score_col].mean() for topic in common_topics])\n",
    "    print(f\"Overall mean group {model_group_2_str}: {overall_mean_2}\")\n",
    "    \n",
    "    stats_per_topic = {}\n",
    "    for topic in common_topics:\n",
    "        stats = {}\n",
    "        group_1_scores = df_grouped_1.get_group(topic)[score_col].values\n",
    "        group_2_scores = df_grouped_2.get_group(topic)[score_col].values\n",
    "        stats[f'{model_group_1_str}_mean'] = group_1_scores.mean()\n",
    "        stats[f'{model_group_2_str}_mean'] = group_2_scores.mean()\n",
    "        stats['score_diff'] = group_1_scores.mean() - group_2_scores.mean()\n",
    "        \n",
    "        if group_1_scores.shape[0] == 1 and group_2_scores.shape[0] == 1:\n",
    "            stats_per_topic[topic] = stats\n",
    "            continue  # Skip topics with only one response per group\n",
    "\n",
    "        # Bootstrapping for confidence intervals\n",
    "        sample_1 = np.random.choice(group_1_scores, size=(n_resamples, group_1_scores.shape[0]), replace=True)\n",
    "        sample_2 = np.random.choice(group_2_scores, size=(n_resamples, group_2_scores.shape[0]), replace=True)\n",
    "        bootstrap_diffs = sample_1.mean(axis=1) - sample_2.mean(axis=1)\n",
    "        stats['score_diff_lb'] = np.percentile(bootstrap_diffs, 100 * ci_alpha / 2)\n",
    "        stats['score_diff_ub'] = np.percentile(bootstrap_diffs, 100 * (1 - ci_alpha / 2))\n",
    "\n",
    "        # Mann-Whitney U test\n",
    "        stats['p'] = scipy.stats.mannwhitneyu(group_1_scores, group_2_scores, alternative='two-sided').pvalue\n",
    "\n",
    "        stats[f'{model_group_1_str}_ste'] = scipy.stats.sem(group_1_scores)\n",
    "        stats[f'{model_group_2_str}_ste'] = scipy.stats.sem(group_2_scores)\n",
    "        stats[f'{model_group_1_str}_count'] = group_1_scores.shape[0]\n",
    "        stats[f'{model_group_2_str}_count'] = group_2_scores.shape[0]\n",
    "        stats['info'] = df_grouped_1.get_group(topic)['short_desc-en'].values[0]\n",
    "\n",
    "        stats_per_topic[topic] = stats\n",
    "\n",
    "    stats_per_topic = pd.DataFrame(stats_per_topic).T\n",
    "    return stats_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pvalues(p_values, method='fisher'):\n",
    "    p_values = p_values[~np.isnan(p_values)]\n",
    "    return scipy.stats.combine_pvalues(p_values, method=method)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_plot_score_diff(df, model_group_1_label, model_group_2_label, overall_mean_diff=None,\n",
    "                           top_k=None, figsize=(10, 20), figname=None, show_info=False, show=True):\n",
    "    df = df.sort_values(\"score_diff\", ascending=False)\n",
    "    if overall_mean_diff is None:\n",
    "        overall_mean_diff = df['score_diff'].mean()\n",
    "    if top_k is not None:\n",
    "        amount_omitted = len(df) - 2 * top_k\n",
    "        gap_df = pd.DataFrame({col: np.nan for col in df.columns}, index=['...'])\n",
    "        df = pd.concat([df.head(top_k), gap_df, df.tail(top_k)])\n",
    "    else:\n",
    "        amount_omitted = 0\n",
    "    if model_group_1_label in model_group_names:\n",
    "        model_group_1_label = model_group_names[model_group_1_label]\n",
    "    if model_group_2_label in model_group_names:\n",
    "        model_group_2_label = model_group_names[model_group_2_label]\n",
    "\n",
    "    error_bounds = np.array([df['score_diff'] - df['score_diff_lb'], df['score_diff_ub'] - df['score_diff']])\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.gca()\n",
    "    sns.pointplot(data=df, y=df.index, x='score_diff', join=False, color='black')\n",
    "    plt.errorbar(y=df.index, x=df['score_diff'], xerr=error_bounds, fmt='none', ecolor='black', capsize=5)\n",
    "    \n",
    "    p_vals_str = df['p'].apply(lambda x: f\"({x:.1e})\" if not pd.isnull(x) else '')\n",
    "    if top_k is None:\n",
    "        plt.yticks(labels=df.index.values + ' ' + p_vals_str, ticks=range(df.shape[0]))\n",
    "        plt.plot([overall_mean_diff, overall_mean_diff], [-0.5, len(df) - 0.5], color='red', linestyle='--')\n",
    "    else:\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        for i, (index, row) in enumerate(df.iterrows()):\n",
    "            if index == '...':\n",
    "                plt.annotate(f'[... {amount_omitted} omitted]', xy=(overall_mean_diff, i), xytext=(0, 2), textcoords='offset points', va='center', ha='center')\n",
    "                continue\n",
    "            elif i < len(df) / 2:\n",
    "                plt.annotate(index + ' ' + p_vals_str.loc[index], xy=(overall_mean_diff, i), xytext=(-15, 0), textcoords='offset points', va='center', ha='right')\n",
    "            else:\n",
    "                plt.annotate(p_vals_str.loc[index] + ' ' + index, xy=(overall_mean_diff, i), xytext=(15, 0), textcoords='offset points', va='center', ha='left')\n",
    "            if show_info:\n",
    "                info = row['info']\n",
    "                plt.annotate(info, xy=(overall_mean_diff, i), xytext=(195, 0), textcoords='offset points', va='center', ha='left', style='italic')\n",
    "        plt.plot([overall_mean_diff, overall_mean_diff], [-0.5, len(df) // 2 - 0.5], color='red', linestyle='--')\n",
    "        plt.plot([overall_mean_diff, overall_mean_diff], [len(df) // 2 + 0.5, len(df) - 0.5], color='red', linestyle='--')\n",
    "    \n",
    "    plt.annotate(f\"{model_group_1_label} rate higher →\", xy=(overall_mean_diff, 0), xytext=(0, 15),\n",
    "                 textcoords='offset points', va='center', ha='center', color='black', fontweight='bold')\n",
    "    plt.annotate(f\"← {model_group_2_label} rate higher\", xy=(overall_mean_diff, len(df) - 1), xytext=(0, -15),\n",
    "                 textcoords='offset points', va='center', ha='center', color='black', fontweight='bold')\n",
    "    plt.ylabel('')\n",
    "    plt.xlabel('Score Difference')\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    ax.tick_params(axis='x', which='both', colors='black', bottom=True, top=True, labeltop=True, labelbottom=True)\n",
    "    x_ticks = plt.xticks(ax.get_xticks().tolist() + [overall_mean_diff])\n",
    "    for i in range(len(x_ticks[1])):\n",
    "        tick_loc = x_ticks[1][i].get_position()[0]\n",
    "        if tick_loc == overall_mean_diff:\n",
    "            if i < len(x_ticks[0]):\n",
    "                x_ticks[0][i]._apply_params(color='red')\n",
    "            x_ticks[1][i].set_visible(False)\n",
    "    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.2f}\"))\n",
    "\n",
    "    p_fishers = combine_pvalues(df['p'].values.astype(float))\n",
    "    print(f\"Combined p-value: {p_fishers:.2e}\")\n",
    "\n",
    "    if figname is not None:\n",
    "        fig_path = os.path.join(FIGURES_DIR, figname)\n",
    "        plt.savefig(fig_path, bbox_inches='tight', transparent=True)\n",
    "        print(f\"Saved figure to {fig_path}\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Scores Between Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of Chinese LLMs in Chinese vs US LLMs in English\n",
    "group_1 = 'chinese-zh'\n",
    "group_2 = 'united-states-en'\n",
    "print(model_groups[group_1])\n",
    "print(model_groups[group_2])\n",
    "\n",
    "stats_for_name_plot = compare_scores(answers, group_1, group_2)\n",
    "\n",
    "# Rename two of the longest names for a nicer plot\n",
    "stats_for_name_plot.rename(index={'Mahammad Amin Rasulzade': 'M.A. Rasulzade'}, \n",
    "                           inplace=True)\n",
    "stats_for_name_plot.rename(index={'Gurbanguly Berdimuhamedow': 'G. Berdimuhamedow'}, \n",
    "                           inplace=True)\n",
    "# Nb: can create an inconsistency in naming if other plots with namesare shown in the paper\n",
    "\n",
    "# Change tags of figures by hands, with tags aiming to balance concision, \n",
    "# neutrality, and explanation content / why the person may be controversial.\n",
    "# Dates provided regard stay in office and particular events.\n",
    "\n",
    "stats_for_name_plot.loc['Klement Gottwald', 'info'] = 'President of USSR-aligned Czechoslovakia (1948-1953)'\n",
    "stats_for_name_plot.loc['Mao Zedong', 'info'] = 'Founder of the PRC & CCP chairman (1943-1976) '\n",
    "stats_for_name_plot.loc['Nikolai Patrushev', 'info'] = 'High-ranking Russian security official (1999-)'\n",
    "stats_for_name_plot.loc['G. Berdimuhamedow', 'info'] = 'President of Turkmenistan (2006-2022)'\n",
    "stats_for_name_plot.loc['Xi Jinping', 'info'] = 'General secretary of the CCP (2012-)'\n",
    "stats_for_name_plot.loc['Ronald Reagan', 'info'] = 'Republican US President (1981-1989)'\n",
    "stats_for_name_plot.loc['Lenín Moreno', 'info'] = 'President of Ecuador (2017-2021)'\n",
    "stats_for_name_plot.loc['Sebastián Piñera', \n",
    "                        'info'] = \"President of Chile (2010-2014, 2018-2022)\"\n",
    "stats_for_name_plot.loc['Ibn Saud', 'info'] = 'First king of Saudi Arabia (1932-1953)'\n",
    "stats_for_name_plot.loc['Alexander Vučić', 'info'] = 'President of the Republic of Serbia (2017-)'\n",
    "stats_for_name_plot.loc['Nicolás Maduro', 'info'] = 'President of Venezuela (2013-)'\n",
    "stats_for_name_plot.loc['Jacques Chirac', 'info'] = 'President of France (1995-2007)'\n",
    "stats_for_name_plot.loc['Hua Guofeng', 'info'] = \"Chinese politician, CCP chairman (1976-1981) & PRC Premier (1976-1980)\"\n",
    "stats_for_name_plot.loc['Nikolai Valuev', 'info'] = 'Russian boxer & United Russia politician'\n",
    "stats_for_name_plot.loc['Andrea Casiraghi', 'info'] = \"Monegasque royal\"\n",
    "stats_for_name_plot.loc['Henry Cavill', 'info'] = 'British actor'\n",
    "stats_for_name_plot.loc['Sergej Naryškin', 'info'] = 'United Russia politician & high-ranking security official'\n",
    "stats_for_name_plot.loc['Li Peng', 'info'] = 'Premier of the PRC (1988-1998)'\n",
    "stats_for_name_plot.loc['Ramzan Kadyrov', 'info'] = 'Head of the Chechen Republic (2007-)'\n",
    "stats_for_name_plot.loc['Sunjong of Korea', 'info'] = 'Korean emperor (1907-1910) before Japanese annexation'\n",
    "stats_for_name_plot.loc['Kōki Hirota', 'info'] = 'Japanese politician, executed for crimes against peace (1948)'\n",
    "stats_for_name_plot.loc['Wilhelm Canaris', 'info'] = 'Nazi military intelligence head, turned regime opponent'\n",
    "stats_for_name_plot.loc['Imre Nagy', 'info'] = 'Hungarian communist & 1956 anti-USSR revolution leader'\n",
    "stats_for_name_plot.loc['Shamil Basayev', 'info'] = 'Chechen anti-Russian guerilla leader'\n",
    "stats_for_name_plot.loc['Mohammad Najibullah', 'info'] = 'Leader of USSR-aligned Afghanistan (1986-1992)'\n",
    "stats_for_name_plot.loc['Ben Carson', 'info'] = 'American Republican politician and neurosurgeon'\n",
    "stats_for_name_plot.loc['George Stinney', 'info'] = 'African-American boy wrongfully executed for murder (1944)'\n",
    "stats_for_name_plot.loc['M.A. Rasulzade', 'info'] = 'Azerbaijani president (1918) & (non-communist) politician'\n",
    "stats_for_name_plot.loc['Yoshiko Kawashima', \n",
    "          'info'] = 'Chinese princess; pro-Japanese spy; WW2 Manchukuo figure'\n",
    "stats_for_name_plot.loc['Alexander Litvinenko', \n",
    "          'info'] = 'FSB defector and Vladimir Putin critic, murdered in the UK'\n",
    "stats_for_name_plot.loc['Rodrigo Lara', 'info'] = 'Columbian lawyer & politician; murdered by Pablo Escobar'\n",
    "stats_for_name_plot.loc['Lee Teng-hui', 'info'] = 'President of Taiwan (1988-2000); KMT politician'\n",
    "stats_for_name_plot.loc['Hu Jia', 'info'] = 'Chinese human-rights activist & dissident'\n",
    "stats_for_name_plot.loc['Tenzin Gyatso', 'info'] = '14th Dalai Lama (1940-)'\n",
    "stats_for_name_plot.loc['Lai Ching-te', 'info'] = 'President of Taiwan (2024-); DPP politician'\n",
    "stats_for_name_plot.loc['Tsai Ing-wen', 'info'] = 'President of Taiwan (2016-2024); DPP politician'\n",
    "stats_for_name_plot.loc['Chen Guangcheng', 'info'] = \"Chinese lawyer, human-rights activist & dissident\"\n",
    "stats_for_name_plot.loc['Wei Jingsheng', 'info'] = \"Chinese human-rights activist & dissident\"\n",
    "stats_for_name_plot.loc['Jimmy Lai', 'info'] = \"Hong Kong opposition politician\"\n",
    "stats_for_name_plot.loc['Joshua Wong', 'info'] = 'Hong Kong politician (Umbrella Movement leader)'\n",
    "stats_for_name_plot.loc['Nathan Law', 'info'] = 'Hong Kong politician (Umbrella Movement leader)'\n",
    "\n",
    "forest_plot_score_diff(stats_for_name_plot,\n",
    "                       group_1, group_2, top_k=20, figsize=(4, 10), \n",
    "                       figname=f\"forest_plot_per_topic_{group_1}-{group_2}.pdf\",\n",
    "                       show_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pairs = (ova_languages + ove_languages + ova_blocks \n",
    "#         +ova_blocks_self_lang + ova_western + ova_chinese) \n",
    "\n",
    "#for group_1, group_2 in pairs:\n",
    "#    diffs = compare_scores(answers, group_1, group_2)\n",
    "#    forest_plot_score_diff(\n",
    "#        diffs, \n",
    "#        group_1, \n",
    "#        group_2, \n",
    "#        top_k=20, \n",
    "#        figsize=(4, 8), \n",
    "#        figname=f\"forest_plot_per_topic_{group_1}-{group_2}.pdf\", \n",
    "#        show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Compare scores by tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the tag CSV files\n",
    "MANIFESTO_TAGS_DIR = PROCESSED_ANSWERS_DIR\n",
    "\n",
    "# Create a dictionary to map category IDs to titles\n",
    "cat_id_to_title = {\n",
    "    '103_Anti-Imperialism': 'Anti-Imperialism \\uf164',\n",
    "    '104_Military: Positive': 'Military \\uf164',\n",
    "    '105_Military: Negative': 'Military \\uf165',\n",
    "    '106_Peace': 'Peace \\uf164',\n",
    "    '107_Internationalism: Positive': 'Internationalism \\uf164',\n",
    "    '108_European Community/Union: Positive': 'European Union \\uf164',\n",
    "    '108_a_United States: Positive': 'United States \\uf164',\n",
    "    '108_b_Russia/USSR/CIS: Positive': 'Russia/USSR \\uf164',\n",
    "    '108_c_China/PRC: Positive': 'China (PRC) \\uf164',\n",
    "    '109_Internationalism: Negative': 'Internationalism \\uf165',\n",
    "    '110_European Community/Union: Negative': 'European Union \\uf165',\n",
    "    '110_a_United States: Negative': 'United States \\uf165',\n",
    "    '110_b_Russia/USSR/CIS: Negative': 'Russia/USSR \\uf165',\n",
    "    '110_c_China/PRC: Negative': 'China (PRC) \\uf165',\n",
    "    '201_Freedom and Human Rights': 'Freedom & Human Rights \\uf164',\n",
    "    '202_Democracy': 'Democracy \\uf164',\n",
    "    '203_Constitutionalism: Positive': 'Constitutional Reform \\uf165',\n",
    "    '204_Constitutionalism: Negative': 'Constitutional Reform \\uf164',\n",
    "    '301_Federalism': 'Federalism \\uf164',\n",
    "    '302_Centralisation': 'Centralisation \\uf164',\n",
    "    '303_Governmental and Administrative Efficiency': 'Efficient Governance \\uf164',\n",
    "    '304_a_Against Political Corruption': 'Anti-Corruption \\uf164',\n",
    "    '304_b_Involved in Political Corruption': 'Involved in Corruption \\uf164',\n",
    "    '305_Political Authority': 'Political Authority \\uf164',\n",
    "    '401_Free Market Economy': 'Free Market \\uf164',\n",
    "    '402_Incentives': 'Supply-side Economics \\uf164',\n",
    "    '403_Market Regulation': 'Market Regulation \\uf164',\n",
    "    '404_Economic Planning': 'Economic Planning \\uf164',\n",
    "    '405_Corporatism/ Mixed Economy': 'Mixed Economy \\uf164',\n",
    "    '406_Protectionism: Positive': 'Protectionism \\uf164',\n",
    "    '407_Protectionism: Negative': 'Protectionism \\uf165',\n",
    "    '408_Economic Goals': 'Economic Goals \\uf164',\n",
    "    '409_Keynesian Demand Management': 'Demand-side Economics \\uf164',\n",
    "    '410_Economic Growth: Positive': 'Economic Growth \\uf164',\n",
    "    '411_Technology and Infrastructure': 'Tech & Infrastructure \\uf164',\n",
    "    '412_Controlled Economy': 'Economic Control \\uf164',\n",
    "    '413_Nationalisation': 'Nationalisation \\uf164',\n",
    "    '414_Economic Orthodoxy': 'Economic Orthodoxy \\uf164',\n",
    "    '415_Marxist Analysis: Positive': 'Marxism \\uf164',\n",
    "    '416_Anti-Growth Economy: Positive': 'Anti-Growth \\uf164',\n",
    "    '501_Environmental Protection: Positive': 'Environmentalism \\uf164',\n",
    "    '502_Culture: Positive': 'Culture \\uf164',\n",
    "    '503_Equality: Positive': 'Equality \\uf164',\n",
    "    '504_Welfare State Expansion': 'Welfare State \\uf164',\n",
    "    '505_Welfare State Limitation': 'Welfare State \\uf165',\n",
    "    '506_Education Expansion': 'State-funded Education \\uf164',\n",
    "    '507_Education Limitation': 'State-funded Education \\uf165',\n",
    "    '601_National Way of Life: Positive': 'National Way of Life \\uf164',\n",
    "    '602_National Way of Life: Negative': 'National Way of Life \\uf165',\n",
    "    '603_Traditional Morality: Positive': 'Traditional Morality \\uf164',\n",
    "    '604_Traditional Morality: Negative': 'Traditional Morality \\uf165',\n",
    "    '605_Law and Order: Positive': 'Law & Order \\uf164',\n",
    "    '606_Civic Mindedness: Positive': 'Civic Mindedness \\uf164',\n",
    "    '607_Multiculturalism: Positive': 'Multiculturalism \\uf164',\n",
    "    '608_Multiculturalism: Negative': 'Multiculturalism \\uf165',\n",
    "    '701_Labour Groups: Positive': 'Worker Rights \\uf164',\n",
    "    '702_Labour Groups: Negative': 'Worker Rights \\uf165',\n",
    "    '703_Agriculture and Farmers: Positive': 'Agriculture & Farmers \\uf164',\n",
    "    '704_Middle Class and Professional Groups': 'Professionals \\uf164',\n",
    "    '705_Underprivileged Minority Groups': 'Minority Groups \\uf164',\n",
    "    '706_Non-economic Demographic Groups': 'Demographic Groups \\uf164'\n",
    "}\n",
    "\n",
    "tags = pd.read_csv(os.path.join(PROCESSED_ANSWERS_DIR, 'tags_clean.csv'))\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load raw tag data\n",
    "(Only need to run once, see tags_clean.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of CSV files to be combined\n",
    "# csv_files = [\n",
    "#     \"manifesto_tagged_topics_economy.csv\",\n",
    "#     \"manifesto_tagged_topics_external_relations.csv\",\n",
    "#     \"manifesto_tagged_topics_fabric_of_society.csv\",\n",
    "#     \"manifesto_tagged_topics_freedom_and_democracy.csv\",\n",
    "#     \"manifesto_tagged_topics_political_system.csv\",\n",
    "#     \"manifesto_tagged_topics_social_groups.csv\",\n",
    "#     \"manifesto_tagged_topics_welfare_and_quality_of_life.csv\"\n",
    "# ]\n",
    "# \n",
    "# # Read each CSV file into a DataFrame\n",
    "# df_list = [pd.read_csv(os.path.join(MANIFESTO_TAGS_DIR, file), low_memory=False) for file in csv_files]\n",
    "# \n",
    "# # Merge the DataFrames on the 'name' column\n",
    "# merged_df = df_list[0]  # Start with the first DataFrame\n",
    "# for i, df in enumerate(df_list[1:], 1):\n",
    "#     merged_df = pd.merge(merged_df, df, on='name', how='outer', suffixes=(f'_left{i}', f'_right{i}'))\n",
    "# \n",
    "# # Drop duplicate columns (those ending with '_left' or '_right')\n",
    "# columns_to_drop = [col for col in merged_df.columns if col.endswith('_left') or col.endswith('_right')]\n",
    "# tags = merged_df.drop(columns=columns_to_drop)\n",
    "# \n",
    "# # Examine the combined DataFrame\n",
    "# print(f\"Columns in the combined tags DataFrame:\\n{tags.columns.tolist()}\\n\")\n",
    "# print(\"First few rows of the tags DataFrame:\")\n",
    "# display(tags.head())\n",
    "# \n",
    "# # Exclude columns that end with '.description'\n",
    "# non_description_cols = [col for col in tags.columns if not col.endswith('.description')]\n",
    "# \n",
    "# # Count non-null values for each non-description column\n",
    "# non_null_counts = tags[non_description_cols].notnull().sum()\n",
    "# \n",
    "# # Create an overview DataFrame\n",
    "# overview_df = pd.DataFrame({\n",
    "#     'Column Name': non_null_counts.index,\n",
    "#     'Non-Null Value Count': non_null_counts.values\n",
    "# })\n",
    "# \n",
    "# # Sort the DataFrame by 'Non-Null Value Count' in descending order\n",
    "# overview_df = overview_df.sort_values(by='Non-Null Value Count', ascending=False).reset_index(drop=True)\n",
    "# \n",
    "# # Display the overview\n",
    "# display(overview_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the minimum frequency threshold for tags to keep\n",
    "# min_freq = 50\n",
    "# \n",
    "# # Identify columns to drop (those with non-null counts below 'min_freq')\n",
    "# rare_cols = non_null_counts[non_null_counts < min_freq].index.tolist()\n",
    "# \n",
    "# print(f\"Dropping {len(rare_cols)} columns with fewer than {min_freq} non-null values.\")\n",
    "# \n",
    "# # Drop rare columns and their corresponding '.explanation' columns\n",
    "# tags = tags.drop(columns=rare_cols, errors='ignore')\n",
    "# tags = tags.drop(columns=[col.replace('.result', '.explanation') for col in rare_cols if col.endswith('.result')], errors='ignore')\n",
    "# \n",
    "# # Remove specific unwanted columns (e.g., those with 'No meaningful category applies')\n",
    "# tags = tags.drop(columns=[col for col in tags.columns if 'No meaningful category applies' in col], errors='ignore')\n",
    "# tags = tags.drop(columns=[col for col in tags.columns if 'Political Corruption' in col and all(x not in col for x in ['Involved', 'Against'])], errors='ignore')\n",
    "# \n",
    "# # Re-check remaining result columns\n",
    "# result_cols = [col for col in tags.columns if col.endswith('.result')]\n",
    "# print(f\"Smallest remaining result column size: {tags[result_cols].count().min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure the mapping includes all remaining categories\n",
    "# # Extract category IDs from the result columns\n",
    "# result_cols = [col for col in tags.columns if col.endswith('.result')]\n",
    "# for result_col in result_cols:\n",
    "#     parts = result_col.split('.')\n",
    "#     if len(parts) >= 2:\n",
    "#         category_name = parts[1]\n",
    "#         if category_name not in cat_id_to_title:\n",
    "#             # If not in the mapping, add it directly\n",
    "#             cat_id_to_title[category_name] = category_name\n",
    "# \n",
    "# # Remove any explanation columns not in the mapping\n",
    "# explanation_cols = [col for col in tags.columns if col.endswith('.explanation')]\n",
    "# invalid_explanation_cols = [col for col in explanation_cols if col.split('.')[1] not in cat_id_to_title]\n",
    "# tags = tags.drop(columns=invalid_explanation_cols, errors='ignore')\n",
    "# \n",
    "# # Drop any remaining '.description' columns\n",
    "# tags = tags.drop(columns=[col for col in tags.columns if col.endswith('.description')], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Drop tags not in the answers df\n",
    "# tags = tags[tags['name'].isin(answers['name-en'].unique())]\n",
    "# tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tags_clean = tags.set_index('name')[[f\"categories.{tag_id}.result\" for tag_id in cat_id_to_title.keys()]]\n",
    "# tags_clean.to_csv(os.path.join(PROCESSED_ANSWERS_DIR, 'tags_clean.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Tags with Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure 'name-en' in answers and 'name' in tags are strings\n",
    "answers['name-en'] = answers['name-en'].astype(str)\n",
    "tags['name'] = tags['name'].astype(str)\n",
    "\n",
    "# Merge the DataFrames on the 'name' columns\n",
    "answers_tagged = answers.merge(tags, left_on='name-en', right_on='name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(answers_tagged))\n",
    "answers_tagged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Topics with no tags\n",
    "no_tags = answers['name-en'].unique()[~np.isin(answers['name-en'].unique(), tags['name'])]\n",
    "print(f\"Topics with no tags: {no_tags}\")\n",
    "print(f\"Number of topics with no tags: {len(no_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Visualize Tag Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the number of occurrences for each category\n",
    "col_true_counts = {}\n",
    "for cat_id, cat_title in cat_id_to_title.items():\n",
    "    true_counts = (tags[f\"categories.{cat_id}.result\"] == True).sum()\n",
    "    col_true_counts[cat_title] = true_counts\n",
    "\n",
    "# Sort the counts in descending order\n",
    "col_true_counts = dict(sorted(col_true_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Plot the frequencies\n",
    "plt.figure(figsize=(8, 11))\n",
    "sns.barplot(x=list(col_true_counts.values()), y=list(col_true_counts.keys()), orient='h', color='skyblue')\n",
    "# plt.xlabel('Number of Occurrences')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'manifesto_tags_frequency.pdf'), bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions for Statistical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def diff(x, y):\n",
    "    \"\"\"Calculate the difference between two arrays.\"\"\"\n",
    "    return x - y\n",
    "\n",
    "def diff_ranks(x, y):\n",
    "    \"\"\"Calculate the difference in ranks between two arrays.\"\"\"\n",
    "    min_ranks_x = scipy.stats.rankdata(x, method='min')\n",
    "    min_ranks_y = scipy.stats.rankdata(y, method='min')\n",
    "    max_ranks_x = scipy.stats.rankdata(x, method='max')\n",
    "    max_ranks_y = scipy.stats.rankdata(y, method='max')\n",
    "    \n",
    "    rank_diffs = np.zeros(x.shape[0])\n",
    "    idx_fully_pos_diff = (min_ranks_x - max_ranks_y) > 0\n",
    "    rank_diffs[idx_fully_pos_diff] = min_ranks_x[idx_fully_pos_diff] - max_ranks_y[idx_fully_pos_diff]\n",
    "    idx_fully_neg_diff = (max_ranks_x - min_ranks_y) < 0\n",
    "    rank_diffs[idx_fully_neg_diff] = max_ranks_x[idx_fully_neg_diff] - min_ranks_y[idx_fully_neg_diff]\n",
    "    return pd.Series(rank_diffs, index=x.index)\n",
    "\n",
    "def compare_scores_tagged(df, model_group_1_str, model_group_2_str, score_col='score', diff_func=diff, ci_alpha=0.05):\n",
    "    \"\"\"Compare scores between two model groups based on tags.\"\"\"\n",
    "    model_group_1 = parse_model_group(model_group_1_str)\n",
    "    model_group_2 = parse_model_group(model_group_2_str)\n",
    "    df_1 = df[df.model.isin(model_group_1)]\n",
    "    df_2 = df[df.model.isin(model_group_2)]\n",
    "    \n",
    "    # Keep only common topics\n",
    "    common_topics = np.intersect1d(df_1['name-en'].unique(), df_2['name-en'].unique())\n",
    "    df_1 = df_1[df_1['name-en'].isin(common_topics)]\n",
    "    df_2 = df_2[df_2['name-en'].isin(common_topics)]\n",
    "    \n",
    "    # Initialize variables\n",
    "    times_std = scipy.stats.norm.ppf(1 - ci_alpha / 2)\n",
    "    stats_per_tag = {}\n",
    "    \n",
    "    for cat_id, cat_title in cat_id_to_title.items():\n",
    "        stats = {}\n",
    "        tag_result_col = f\"categories.{cat_id}.result\"\n",
    "        group_1_means = df_1[df_1[tag_result_col] == True].groupby('name-en')[score_col].mean()\n",
    "        group_2_means = df_2[df_2[tag_result_col] == True].groupby('name-en')[score_col].mean()\n",
    "        score_diff = diff_func(group_1_means, group_2_means)\n",
    "        \n",
    "        stats[f'{model_group_1_str}_mean'] = group_1_means.mean()\n",
    "        stats[f'{model_group_2_str}_mean'] = group_2_means.mean()\n",
    "        stats['score_diff'] = score_diff.mean()\n",
    "        score_diff_sem = score_diff.sem()\n",
    "        stats['score_diff_lb'] = score_diff.mean() - times_std * score_diff_sem\n",
    "        stats['score_diff_ub'] = score_diff.mean() + times_std * score_diff_sem\n",
    "        stats['count'] = group_1_means.shape[0]\n",
    "        \n",
    "        if stats['count'] < 20:\n",
    "            continue  # Skip categories with insufficient data\n",
    "        \n",
    "        # Comparison with topics not in the tag\n",
    "        group_1_means_comp = df_1[df_1[tag_result_col] != True].groupby('name-en')[score_col].mean()\n",
    "        group_2_means_comp = df_2[df_2[tag_result_col] != True].groupby('name-en')[score_col].mean()\n",
    "        score_diff_comp = diff_func(group_1_means_comp, group_2_means_comp)\n",
    "        test = scipy.stats.ttest_ind(score_diff, score_diff_comp, alternative='two-sided', equal_var=False)\n",
    "        stats['p'] = test.pvalue\n",
    "        \n",
    "        stats['cat_title'] = cat_title\n",
    "        stats_per_tag[cat_title] = stats\n",
    "        \n",
    "    result_df = pd.DataFrame(stats_per_tag).T\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Scores Between Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_1 = \"en\"\n",
    "group_2 = \"zh\"\n",
    "# group_1 = \"en\"\n",
    "# group_2 = 'ru'\n",
    "# group_1 = 'xai/grok-beta-en'\n",
    "# group_2 = 'western-en, not-xai/grok-beta'\n",
    "\n",
    "diffs = compare_scores_tagged(answers_tagged, group_1, group_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest_plot_score_diff(diffs, group_1, group_2, figsize=(4, 6), top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pairs = (ova_languages + ove_languages + ova_blocks \n",
    "#         +ova_blocks_self_lang + ova_webstern + ova_chinese) \n",
    "\n",
    "pairs = ova_western + ova_chinese\n",
    "\n",
    "for group_1, group_2 in pairs:\n",
    "    print(f\"Comparing {group_1} vs {group_2}\")\n",
    "    print(model_groups[group_1], model_groups[group_2])\n",
    "    diffs = compare_scores_tagged(answers_tagged, group_1, group_2)\n",
    "    forest_plot_score_diff(diffs, group_1, group_2, figsize=(4, 6), top_k=10, figname=f\"forest_plot_per_tag_{group_1}-{group_2}.pdf\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models Using Combined P-Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a dictionary to store combined p-values per model\n",
    "# p_combined_by_model = {}\n",
    "# \n",
    "# # Filter the tagged answers to include only English responses\n",
    "# answers_tagged_en = answers_tagged[answers_tagged.language_code == 'en']\n",
    "# \n",
    "# # Get the list of all English models\n",
    "# all_models_en = answers_tagged_en.model.unique()\n",
    "# \n",
    "# # For each model, compare it to all other models\n",
    "# for model in all_models_en:\n",
    "#     # Compute the differences between the model and all other models\n",
    "#     diffs = compare_scores_tagged(\n",
    "#         answers_tagged_en,\n",
    "#         model,\n",
    "#         list(all_models_en[all_models_en != model])\n",
    "#     )\n",
    "#     # Combine the p-values using Fisher's method\n",
    "#     p_combined_by_model[model] = combine_pvalues(diffs['p'].values.astype(float))\n",
    "# \n",
    "# # Print the combined p-values for each model\n",
    "# print(\"Combined p-values for each model:\")\n",
    "# for model, p_value in p_combined_by_model.items():\n",
    "#     print(f\"{model}: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Display the combined p-values sorted in ascending order\n",
    "# combined_p_values_series = pd.Series(p_combined_by_model).sort_values()\n",
    "# print(\"\\nCombined p-values sorted in ascending order:\")\n",
    "# display(combined_p_values_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Compute and Visualize Tag-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_result_cols = [f\"categories.{tag_id}.result\" for tag_id in cat_id_to_title.keys()]\n",
    "# other_cols = ['model', 'score']\n",
    "# tag_scores = answers_tagged.loc[:, tag_result_cols + other_cols]\n",
    "# tag_scores = tag_scores.rename(columns=dict(zip(tag_result_cols, cat_id_to_title.values())))\n",
    "# tag_result_cols = list(cat_id_to_title.values())\n",
    "# tag_scores[tag_result_cols] = tag_scores[tag_result_cols].astype(float)\n",
    "# tag_scores[tag_result_cols] = tag_scores[tag_result_cols].apply(lambda col: col[col == 1.] * tag_scores['score'])\n",
    "# tag_scores[tag_result_cols] = tag_scores[tag_result_cols].subtract(tag_scores[tag_result_cols].mean(axis=0))\n",
    "# tag_means = tag_scores.groupby('model')[tag_result_cols].mean()\n",
    "\n",
    "def compute_tag_means(model_group_strs=None, only_common_names=False):\n",
    "    if model_group_strs is None:\n",
    "        model_group_strs = answers_tagged['model'].unique()\n",
    "    _tag_means = {}    \n",
    "    for tag_id in cat_id_to_title.keys():\n",
    "        tag_id_str = f\"categories.{tag_id}.result\"\n",
    "        answers_tagged_true = answers_tagged[answers_tagged[tag_id_str] == True]\n",
    "        tag_mean_by_group = {}\n",
    "        for model_group_str in model_group_strs:\n",
    "            model_group = parse_model_group(model_group_str)\n",
    "            model_group_scores = answers_tagged_true[answers_tagged_true['model'].isin(model_group)]\n",
    "            tag_mean = model_group_scores.groupby('name-en')['score'].mean()\n",
    "            tag_mean_by_group[model_group_str] = tag_mean\n",
    "        tag_mean = pd.concat(tag_mean_by_group, axis=1)\n",
    "        if only_common_names:\n",
    "            tag_mean = tag_mean.dropna()\n",
    "        tag_mean = tag_mean.mean()\n",
    "        tag_id = cat_id_to_title[tag_id]\n",
    "        _tag_means[tag_id] = tag_mean\n",
    "    return pd.DataFrame(_tag_means)\n",
    "tag_means = compute_tag_means(only_common_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_models_total = answers_tagged['model'].nunique()\n",
    "# topics_rated_by_all = answers_tagged.groupby(['name-en', 'language_code'])['model'].count()\n",
    "# (topics_rated_by_all.reset_index().groupby(['name-en'])['model'].max() == 19).sum()\n",
    "# topics_rated_by_all = topics_rated_by_all[(topics_rated_by_all == num_models_total)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap of Tag Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tag means as a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(tag_means, cmap='coolwarm', center=0.5, cbar_kws={'label': 'Mean Score'})\n",
    "plt.title('Mean Scores per Tag and Model')\n",
    "plt.xlabel('Tag')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centered Heatmap\n",
    "To better understand the relative differences, we center the tag means per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Center the tag means per model\n",
    "tag_means_centered = tag_means.apply(lambda x: x - x.mean(), axis=1)\n",
    "\n",
    "# Visualize the centered tag means\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(tag_means_centered, cmap='coolwarm', center=0, cbar_kws={'label': 'Centered Mean Score'})\n",
    "plt.title('Centered Mean Scores per Tag and Model')\n",
    "plt.xlabel('Tag')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA (OLD)\n",
    "pca = PCA(random_state=0)\n",
    "tag_means_centered = tag_means.apply(lambda x: x - x.mean(), axis=1)\n",
    "tag_means_pca = pca.fit_transform(tag_means_centered.values)[:,:2]\n",
    "tag_means_pca = pd.DataFrame(tag_means_pca, index=tag_means.index, columns=['PC1', 'PC2'])\n",
    "tag_means_pca = tag_means_pca.reset_index().rename(columns={'index': 'model'})\n",
    "pca_components = pca.components_[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import constants\n",
    "importlib.reload(constants)\n",
    "from constants import model_org, model_abbreviations\n",
    "# 8, 9 Mistral. 13, 14 Llama \n",
    "shape_palette = ['P', 'X', '*', '$\\u2b53$', 'h', '^', '>', 'v', '<', '$\\u25e7$', '$\\u25e8$', '$\\u29d3$', '$\\u29d7$',\n",
    "                 '$\\u2660$', '$\\u2663$', '$\\u25b0$', '$\\u2666$', '$\\u273d$', '$\\u25d8$']\n",
    "model_shape = dict(zip(np.unique(list(model_abbreviations.values())), shape_palette))\n",
    "# Define shapes and colors for models based on language and organization\n",
    "language_shape = {\n",
    "    'ar': 'o', \n",
    "    'zh': 'P', \n",
    "    'en': 's',\n",
    "    'fr': 'X',\n",
    "    'ru': 'D',\n",
    "    'es': 'v'\n",
    "}\n",
    "language_name = {\n",
    "    'ar': 'Arabic',\n",
    "    'zh': 'Chinese',\n",
    "    'en': 'English',\n",
    "    'fr': 'French',\n",
    "    'ru': 'Russian',\n",
    "    'es': 'Spanish',\n",
    "}\n",
    "language_colors = {\n",
    "    'ar': '#e41a1c',\n",
    "    'zh': '#377eb8',\n",
    "    'en': '#4daf4a',\n",
    "    'fr': '#984ea3',\n",
    "    'ru': '#ff7f00',\n",
    "    'es': '#ffff33'\n",
    "} # Colors taken from https://colorbrewer2.org/#type=qualitative&scheme=Set1&n=6\n",
    "language_name_colors= {\n",
    "    'Arabic': '#e41a1c',\n",
    "    'Chinese': '#377eb8',\n",
    "    'English': '#4daf4a',\n",
    "    'French': '#984ea3',\n",
    "    'Russian': '#ff7f00',\n",
    "    'Spanish': '#ffff33'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Calculate the magnitude of PCA components\n",
    "pca_components_magnitude = np.linalg.norm(pca_components, axis=0)\n",
    "\n",
    "# Select the top k tags based on their loading magnitudes\n",
    "k = 30\n",
    "top_k_idx = np.argsort(pca_components_magnitude)[-k:]\n",
    "\n",
    "# Scaling factor for model points\n",
    "model_scale = 4.5\n",
    "# model_scale = 1.65\n",
    "# model_scale = 0.8\n",
    "\n",
    "# Plot the PCA components (arrows representing tags)\n",
    "texts = []\n",
    "for i in top_k_idx:\n",
    "    tag_id = tag_means.columns[i]\n",
    "    loading_x = pca_components[0, i] / pca_components_magnitude[i]\n",
    "    loading_y = pca_components[1, i] / pca_components_magnitude[i]\n",
    "    plt.arrow(0, 0, loading_x, loading_y, color='black', alpha=0.2, linewidth=7 * pca_components_magnitude[i],\n",
    "              head_width=0.02)\n",
    "    # loading_x = pca_components[0, i] \n",
    "    # loading_y = pca_components[1, i]\n",
    "    # plt.arrow(0, 0, loading_x, loading_y, color='black', alpha=0.4, linewidth=1, linestyle='dotted')\n",
    "    \n",
    "    # Determine text alignment and rotation\n",
    "    angle = np.degrees(np.arctan2(pca_components[1, i], pca_components[0, i]))\n",
    "    va = 'center'\n",
    "    if angle > 90:\n",
    "        angle -= 180\n",
    "        ha = 'right'\n",
    "    elif angle < -90:\n",
    "        angle += 180\n",
    "        ha = 'right'\n",
    "    else:\n",
    "        ha = 'left'\n",
    "        \n",
    "    if tag_id in [\n",
    "        'Constitutional Reform \\uf164',\n",
    "        # 'Nationalisation \\uf164',\n",
    "        # 'Demographic Groups \\uf164',\n",
    "        'Equality \\uf164',\n",
    "        'Multiculturalism \\uf165',\n",
    "        'European Union \\uf165',\n",
    "        'Worker Rights \\uf165',\n",
    "        'Supply-side Economics \\uf164',\n",
    "        'United States \\uf165',\n",
    "        'Centralisation \\uf164',\n",
    "    ]:\n",
    "        va = 'bottom'\n",
    "    elif tag_id in [\n",
    "        'Minority Groups \\uf164',\n",
    "        'Freedom & Human Rights \\uf164',\n",
    "        'Involved in Corruption \\uf164',\n",
    "        'Internationalism \\uf165',\n",
    "        'Demand-side Economics \\uf164',\n",
    "        'Professionals \\uf164',\n",
    "    ]:\n",
    "        va = 'top'\n",
    "    # elif tag_id in [\n",
    "    #     'China (PRC) \\uf165',\n",
    "    #     'State-funded Education \\uf165',\n",
    "    #     'Supply-side Economics \\uf164',\n",
    "    # ]:\n",
    "    #     ha = 'right' if ha == 'left' else 'left'\n",
    "    #     ha = 'center'\n",
    "    #     va = 'bottom'\n",
    "    \n",
    "    loading_x = loading_x * 1.06 #+ (loading_x / pca_components_magnitude[i]) * 0.01, \n",
    "    loading_y = loading_y * 1.06 #+ (loading_y / pca_components_magnitude[i]) * 0.01, \n",
    "    \n",
    "    if tag_id in [\n",
    "        'Internationalism \\uf165',\n",
    "        'Russia/USSR \\uf164',\n",
    "        'United States \\uf165',\n",
    "    ]:\n",
    "        loading_x -= 0.015\n",
    "        loading_y -= 0.03\n",
    "    elif tag_id in [\n",
    "        'Nationalisation \\uf164',\n",
    "        'Constitutional Reform \\uf164',\n",
    "    ]:\n",
    "        loading_x += 0.01\n",
    "        loading_y += 0.02\n",
    "    \n",
    "    # Annotate the tags\n",
    "    t = plt.text(loading_x, loading_y,\n",
    "                 tag_id, rotation=angle, rotation_mode='anchor', ha=ha, va=va, fontsize=15, alpha=1)\n",
    "    texts.append(t)\n",
    "    \n",
    "# Set aspect ratio to equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "ax = plt.gca()\n",
    "\n",
    "# Define model to organization mapping\n",
    "import sys\n",
    "sys.path.append(\"../ideological-spectrum-llms\")\n",
    "from constants import model_org\n",
    "\n",
    "tag_means_pca['model_root'] = tag_means_pca['model'].apply(lambda x: x.removesuffix(f'-{x.split(\"-\")[-1]}').lower())\n",
    "tag_means_pca['org'] = tag_means_pca['model_root'].apply(lambda x: model_org.get(x, 'Unknown'))\n",
    "tag_means_pca['language'] = tag_means_pca['model'].apply(lambda x: language_name[x.split('-')[-1]])\n",
    "tag_means_pca['model_abbr'] = tag_means_pca['model_root'].apply(lambda x: model_abbreviations[x])\n",
    "abbr_to_org = {model_abbreviations[model]: model_org[model] for model in tag_means_pca['model_root'].unique()}\n",
    "\n",
    "# Plot both\n",
    "vals_by_both = (tag_means_pca.groupby(['model_abbr', 'language'])[['PC1', 'PC2']].mean() * model_scale).reset_index()\n",
    "plot_both = sns.scatterplot(data=vals_by_both, x='PC1', y='PC2', style='model_abbr', hue='language', \n",
    "                            palette=language_name_colors, markers=model_shape, s=150, edgecolor='black', \n",
    "                            ax=ax, alpha=0.25, legend=False)\n",
    "# for i, row in vals_by_both.iterrows():\n",
    "#     plt.text(row['PC1'], row['PC2'], model_org_shape[row['org']], fontsize=14, color=language_colors[row['language']])\n",
    "\n",
    "# Plot by lang\n",
    "delta = 0.01\n",
    "vals_by_lang = (tag_means_pca.groupby('language')[['PC1', 'PC2']].mean() * model_scale).reset_index()\n",
    "plot_lang = sns.scatterplot(data=vals_by_lang, x='PC1', y='PC2', hue='language', palette=language_name_colors, \n",
    "                            markers=model_shape, s=150, edgecolor='black', ax=ax, alpha=1)\n",
    "# for i, row in vals_by_lang.iterrows():\n",
    "#     plt.text(row['PC1'] + delta, row['PC2'] - delta, row['language'], fontsize=14)\n",
    "\n",
    "# Plot by model_abbr\n",
    "vals_by_org = (tag_means_pca.groupby('model_abbr')[['PC1', 'PC2']].mean() * model_scale).reset_index()\n",
    "# vals_by_org[['PC1', 'PC2']] = vals_by_org[['PC1', 'PC2']] / vals_by_org[['PC1', 'PC2']].apply(np.linalg.norm, axis=1)\n",
    "plot_org = sns.scatterplot(data=vals_by_org, x='PC1', y='PC2', style='model_abbr', markers=model_shape, s=150, \n",
    "                           edgecolor='black', color='grey', ax=ax, alpha=1)\n",
    "# for i, row in vals_by_org.iterrows():\n",
    "#     plt.text(row['PC1'] + delta, row['PC2'] - delta, row['org'], fontsize=14)\n",
    "\n",
    "# Remove axes and borders for a cleaner look\n",
    "plt.axis('off')\n",
    "\n",
    "handles, labels = plot_lang.get_legend_handles_labels()\n",
    "legend_lang = plt.legend(handles=handles[:len(language_colors)], labels=labels[:len(language_colors)],\n",
    "                         loc='upper center', ncols=len(language_colors), frameon=False, bbox_to_anchor=(0.5, 1.42), \n",
    "                         fontsize=13, title='Language (average)')\n",
    "handles, labels = plot_org.get_legend_handles_labels()\n",
    "labels = [f'{l} ({abbr_to_org[l]})' for l in labels[len(language_colors):]]\n",
    "legend_org = plt.legend(handles=handles[len(language_colors):], labels=labels,\n",
    "                        loc='lower center', ncols=4, frameon=False, bbox_to_anchor=(0.5, -0.6),\n",
    "                        fontsize=13, title='Model (average)')\n",
    "plt.gca().add_artist(legend_lang)\n",
    "\n",
    "\n",
    "# Save and display the plot\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'tag_means_pca.pdf'), bbox_inches='tight', transparent=True, \n",
    "            bbox_extra_artists=(legend_lang, legend_org))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize PCA Components Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA components alone\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(pca_components[0], pca_components[1])\n",
    "\n",
    "# Annotate each point with the tag title\n",
    "for i, tag_id in enumerate(tag_means.columns):\n",
    "    plt.text(pca_components[0, i], pca_components[1, i], tag_id, fontsize=14)\n",
    "\n",
    "plt.title('PCA Components of Tags')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radar Plot of Tag Means\n",
    "Another way to visualize the differences between model groups is to use radar charts (also known as spider plots). We can compare two model groups across all tags.\n",
    "\n",
    "#### Define Radar Chart Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.transforms import Affine2D\n",
    "\n",
    "def radar_factory(num_vars, frame='circle'):\n",
    "    \"\"\"Create a radar chart with `num_vars` axes.\"\"\"\n",
    "    # calculate evenly-spaced axis angles\n",
    "    theta = np.linspace(0, 2 * np.pi, num_vars, endpoint=False)\n",
    "\n",
    "    class RadarTransform(PolarAxes.PolarTransform):\n",
    "        # Code for custom radar transform\n",
    "        def transform_path_non_affine(self, path):\n",
    "            if path._interpolation_steps > 1:\n",
    "                path = path.interpolated(num_vars)\n",
    "            return Path(self.transform(path.vertices), path.codes)\n",
    "\n",
    "    class RadarAxes(PolarAxes):\n",
    "        name = 'radar'\n",
    "        PolarTransform = RadarTransform\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # Rotate plot such that the first axis is at the top\n",
    "            self.set_theta_zero_location('N')\n",
    "\n",
    "        def fill(self, *args, closed=True, **kwargs):\n",
    "            \"\"\"Override fill to ensure the path is closed.\"\"\"\n",
    "            return super().fill(closed=closed, *args, **kwargs)\n",
    "\n",
    "        def plot(self, *args, **kwargs):\n",
    "            \"\"\"Override plot to ensure the path is closed.\"\"\"\n",
    "            lines = super().plot(*args, **kwargs)\n",
    "            for line in lines:\n",
    "                self._close_line(line)\n",
    "\n",
    "        def _close_line(self, line):\n",
    "            x, y = line.get_data()\n",
    "            if x[0] != x[-1]:\n",
    "                x = np.concatenate([x, [x[0]]])\n",
    "                y = np.concatenate([y, [y[0]]])\n",
    "                line.set_data(x, y)\n",
    "\n",
    "        def set_varlabels(self, labels):\n",
    "            self.set_thetagrids(np.degrees(theta), labels)\n",
    "\n",
    "        def _gen_axes_patch(self):\n",
    "            if frame == 'circle':\n",
    "                return Circle((0.5, 0.5), 0.5)\n",
    "            elif frame == 'polygon':\n",
    "                return RegularPolygon((0.5, 0.5), num_vars, radius=.5, edgecolor=\"k\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown frame type: {frame}\")\n",
    "\n",
    "        def _gen_axes_spines(self):\n",
    "            if frame == 'circle':\n",
    "                return super()._gen_axes_spines()\n",
    "            elif frame == 'polygon':\n",
    "                spine = Spine(\n",
    "                    axes=self,\n",
    "                    spine_type='circle',\n",
    "                    path=Path.unit_regular_polygon(num_vars)\n",
    "                )\n",
    "                spine.set_transform(Affine2D().scale(.5).translate(.5, .5) + self.transAxes)\n",
    "                return {'polar': spine}\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown frame type: {frame}\")\n",
    "\n",
    "    register_projection(RadarAxes)\n",
    "    return theta\n",
    "\n",
    "def radar_plot(model_group_strs, idx_sorted=None, figname=None, annotate_fn=None, str_map=None, color_map=None, group_tag_means=False):\n",
    "    if group_tag_means:\n",
    "        tag_means_per_group = compute_tag_means(model_group_strs, only_common_names=True).T\n",
    "    else:\n",
    "        tag_means_per_group = {}\n",
    "        for model_group_str in model_group_strs:\n",
    "            model_group = parse_model_group(model_group_str)\n",
    "            tag_means_per_group[model_group_str] = tag_means.loc[model_group].mean()\n",
    "        tag_means_per_group = pd.DataFrame(tag_means_per_group)\n",
    "    \n",
    "    tag_means_per_group -= tag_means_per_group.mean()\n",
    "    tag_means_per_group = tag_means_per_group.apply(lambda x: x - x.mean(), axis=1)\n",
    "    tag_means_per_group_back = tag_means_per_group.copy()\n",
    "    \n",
    "    tag_means_per_group = tag_means_per_group.reset_index(names='tag')\n",
    "    \n",
    "    if idx_sorted is None:\n",
    "        idx_to_sort = tag_means_per_group.index.to_list()\n",
    "        idx_sorted = []\n",
    "        for i in range(1, len(idx_to_sort)):\n",
    "            if i == 1:\n",
    "                _tag_means_sorted = tag_means_per_group.drop(columns=['tag'])\n",
    "                smoothness = -_tag_means_sorted.diff(axis=0).abs()\n",
    "                most_smooth_idx = smoothness.mean(axis=1).idxmax()\n",
    "                idx_sorted.append(most_smooth_idx - 1)\n",
    "                idx_to_sort.remove(most_smooth_idx - 1)\n",
    "                idx_sorted.append(most_smooth_idx)\n",
    "                idx_to_sort.remove(most_smooth_idx)\n",
    "            else:\n",
    "                last_vals = tag_means_per_group.drop(columns=['tag']).loc[idx_sorted[-1]]\n",
    "                smoothness = -(last_vals - tag_means_per_group.drop(columns=['tag']).loc[idx_to_sort]).abs()\n",
    "                most_smooth_idx = smoothness.mean(axis=1).idxmax()\n",
    "                idx_sorted.append(most_smooth_idx)\n",
    "                idx_to_sort.remove(most_smooth_idx)\n",
    "        assert len(idx_sorted) == len(tag_means_per_group)\n",
    "        assert len(idx_to_sort) == 0\n",
    "    tag_means_per_group = tag_means_per_group.loc[idx_sorted]\n",
    "    tag_means_per_group = tag_means_per_group.set_index('tag').T\n",
    "            \n",
    "    # Create radar chart\n",
    "    radar = radar_factory(len(cat_id_to_title), frame='polygon')\n",
    "    fig, ax = plt.subplots(figsize=(7, 7), subplot_kw=dict(projection='radar'))\n",
    "    # Set variable labels\n",
    "    ax.axhline(0, color='black', linewidth=1, linestyle='--')\n",
    "    # plt.hlines(0, 0, 2 * np.pi, colors='gray', linestyles='dashed')\n",
    "    for model_group_str in model_group_strs:\n",
    "        tag_means_group = tag_means_per_group.loc[model_group_str]\n",
    "        if color_map is not None:\n",
    "            color = color_map[model_group_str]\n",
    "        else:\n",
    "            color = None\n",
    "        if str_map is not None:\n",
    "            model_group_str = str_map[model_group_str]\n",
    "        ax.plot(radar, tag_means_group, label=model_group_str, color=color)\n",
    "        # ax.fill(theta, tag_means_group, alpha=0.25)\n",
    "    \n",
    "    labels = tag_means_per_group.columns\n",
    "    ax.set_varlabels(labels)\n",
    "    \n",
    "    plt.gcf().canvas.draw()\n",
    "    angles = np.linspace(0,2*np.pi,len(ax.get_xticklabels())+1)\n",
    "    angles = np.rad2deg(angles)\n",
    "    labels = []\n",
    "    for label, angle in zip(ax.get_xticklabels(), angles):\n",
    "        x,y = label.get_position()\n",
    "        ha = 'right'\n",
    "        va = 'center'\n",
    "        angle -= 90\n",
    "        if angle > 90:\n",
    "            angle -= 180\n",
    "            ha = 'left'\n",
    "        lab = ax.text(x,y+0.03, label.get_text(), transform=label.get_transform(), rotation_mode='anchor',\n",
    "                      ha=ha, va=va, rotation=angle)\n",
    "        labels.append(lab)\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    if annotate_fn is None:\n",
    "        plt.legend(loc='upper center', ncols=len(model_group_strs), frameon=False, bbox_to_anchor=(0.5, 1.41), fontsize=13)\n",
    "    else:\n",
    "        annotate_fn(model_group_strs, ax)\n",
    "    \n",
    "    if figname is not None:\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, figname), bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "    return tag_means_per_group_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare by language\n",
    "idx_sorted = [0, 12, 6, 49, 43, 55, 20, 47, 37, 16, 25, 8, 1, 29, 44, 7, 36, 19, 56, 22, 54, 17, 11, 9, 46, 13, 48, 10, 35, 27, 31, 23, 33, 38, 2, 57, 28, 51, 24, 30, 34, 26, 18, 32, 5, 39, 58, 40, 60, 59, 41, 53, 42, 3, 14, 52, 45, 4, 15, 50, 21]\n",
    "tag_means_per_group = radar_plot(language_name.keys(), idx_sorted=idx_sorted, figname='radar_lang.pdf', str_map=language_name, color_map=language_colors, group_tag_means=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_groups['western'] = model_groups['united-states'] + model_groups['european'] + model_groups['israeli']\n",
    "model_groups['russian_strict'] = [col for col in model_groups['russian'] if 'Vikhr' not in col]\n",
    "model_groups['chinese_strict'] = [col for col in model_groups['chinese'] if 'Qwen' not in col]\n",
    "model_groups['western-west'] =  model_groups['united-states-en'] + model_groups['european-en-fr-es'] + model_groups['israeli-en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bloc_name = {\n",
    "    'arabic': 'Arabic Countries',\n",
    "    \"chinese\": 'China (PRC)',\n",
    "    # 'european': 'European Union',\n",
    "    \"russian\": 'Russia',\n",
    "    # 'israeli': 'Israel',\n",
    "    # 'united-states': 'United States'\n",
    "    'western': 'Western Countries'\n",
    "}\n",
    "bloc_color = {\n",
    "    'arabic': language_colors['ar'],\n",
    "    \"chinese\": language_colors['zh'],\n",
    "    # 'european': '#a65628',\n",
    "    \"russian\": language_colors['ru'],\n",
    "    # 'israeli': '#f781bf',\n",
    "    # 'united-states': language_colors['en'],\n",
    "    'western': language_colors['en']\n",
    "}\n",
    "# bloc_name = {\n",
    "#     'arabic-ar': 'Arabic Countries',\n",
    "#     \"chinese-zh\": 'China (PRC)',\n",
    "#     # 'european': 'European Union',\n",
    "#     \"russian-ru\": 'Russia',\n",
    "#     # 'israeli': 'Israel',\n",
    "#     # 'united-states': 'United States'\n",
    "#     'western-west': 'Western Countries'\n",
    "# }\n",
    "# bloc_color = {\n",
    "#     'arabic-ar': language_colors['ar'],\n",
    "#     \"chinese-zh\": language_colors['zh'],\n",
    "#     # 'european': '#a65628',\n",
    "#     \"russian-ru\": language_colors['ru'],\n",
    "#     # 'israeli': '#f781bf',\n",
    "#     # 'united-states': language_colors['en'],\n",
    "#     'western-west': language_colors['en']\n",
    "# }\n",
    "# idx_sorted = [0, 6, 49, 57, 50, 39, 41, 59, 53, 14, 42, 60, 52, 3, 58, 45, 4, 21, 40, 5, 15, 43, 55, 47, 18, 16, 34, 8, 46, 38, 29, 11, 10, 54, 22, 56, 19, 17, 30, 1, 51, 9, 35, 7, 36, 27, 31, 23, 24, 37, 44, 26, 25, 32, 28, 33, 20, 2, 12, 48, 13]\n",
    "idx_sorted = [0, 13, 48, 12, 2, 20, 33, 28, 32, 25, 26, 44, 31, 27, 23, 24, 37, 1, 51, 35, 9, 30, 17, 19, 56, 22, 54, 10, 36, 7, 11, 29, 38, 46, 8, 34, 16, 18, 47, 55, 43, 15, 5, 40, 58, 45, 52, 60, 42, 59, 41, 53, 14, 3, 4, 21, 39, 50, 57, 49, 6]\n",
    "tag_means_per_group = radar_plot(bloc_name.keys(), idx_sorted=idx_sorted, figname='radar_bloc.pdf', str_map=bloc_name, color_map=bloc_color, group_tag_means=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import t-SNE from scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE on tag means\n",
    "tsne = TSNE(n_components=2, perplexity=10, random_state=42)\n",
    "tag_means_tsne = tsne.fit_transform(tag_means.values)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "df = pd.DataFrame(tag_means_tsne, columns=['t-SNE Dimension 1', 't-SNE Dimension 2'])\n",
    "df['model'] = tag_means.index\n",
    "df['model_root'] = df['model'].apply(lambda x: x.removesuffix(f'-{x.split(\"-\")[-1]}').lower())\n",
    "df['org'] = df['model_root'].apply(lambda x: model_org.get(x, 'Unknown'))\n",
    "df['language'] = df['model'].apply(lambda x: x.split('-')[-1])\n",
    "sns.scatterplot(data=df, x='t-SNE Dimension 1', y='t-SNE Dimension 2', hue='language', style='org', palette=language_colors, markers=model_org_shape, s=200, edgecolor='black')\n",
    "\n",
    "# plt.scatter(tag_means_tsne[:, 0], tag_means_tsne[:, 1])\n",
    "\n",
    "# Annotate each point with the model name\n",
    "for i, model in enumerate(tag_means.index):\n",
    "    plt.annotate(model, (tag_means_tsne[i, 0], tag_means_tsne[i, 1]), fontsize=12)\n",
    "\n",
    "plt.title('t-SNE Visualization of Models Based on Tag Means')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering and Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import linkage and dendrogram from scipy\n",
    "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# \n",
    "# # Perform hierarchical clustering\n",
    "# Z = linkage(tag_means.T)\n",
    "# \n",
    "# # Plot the dendrogram\n",
    "# plt.figure(figsize=(10, 15))\n",
    "# dendrogram(Z, labels=[cat_id_to_title[cat_id] for cat_id in tag_means.columns], orientation='right')\n",
    "# plt.title('Dendrogram of Tags Based on Model Scores')\n",
    "# plt.xlabel('Distance')\n",
    "# plt.ylabel('Tag')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ideological_llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
